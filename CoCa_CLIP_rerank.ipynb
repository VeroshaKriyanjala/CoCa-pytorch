{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a8f64e",
   "metadata": {},
   "source": [
    "\n",
    "# CoCa + CLIP Reranking: Hybrid Captioning Notebook\n",
    "\n",
    "This notebook implements your **inference-time reranking** pipeline:\n",
    "\n",
    "1. **Candidate Generation** â€” Generate `N` candidate captions for an image using **CoCa** (via `open_clip`) when available.  \n",
    "   As a robust fallback, we use **BLIP** (Hugging Face `transformers`) to generate `N` candidates with nucleus sampling.\n",
    "2. **CLIPScore Computation** â€” Compute **cosine similarity** between CLIP image and text embeddings.\n",
    "3. **Hybrid Scoring** â€” Compute `Score(c) = log P(CoCa|BLIP)(c | I) + Î± * CLIPScore(I, c)`.\n",
    "4. **Caption Selection** â€” Choose the caption with the highest hybrid score.\n",
    "\n",
    "> âš ï¸ **Note**: This notebook is designed to run both with and without internet.  \n",
    "> - With internet: it will `pip install` missing deps and download models.  \n",
    "> - Without internet: you can still run **CLIP-only reranking** on your **own candidate list** (e.g., from CoCa in your local repo).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb0f70",
   "metadata": {},
   "source": [
    "\n",
    "## Quick Start\n",
    "\n",
    "**Option A â€” Full pipeline (internet available):**\n",
    "1. Run **Setup** to install/load packages.\n",
    "2. In **Config**, set `GENERATOR_BACKEND = \"coca\"` (preferred) or `\"blip\"` (fallback).\n",
    "3. Run **Demo** on your image(s).\n",
    "\n",
    "**Option B â€” Rerank-only (no internet; you already have candidates):**\n",
    "1. Skip installs if packages are present.\n",
    "2. In **Provide Your Own Candidates**, paste your list of candidates per image.\n",
    "3. Run **Rerank + Select** to get the best caption per image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcde37d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ff3fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If you're offline or already have these installed, you can skip the pip cells safely.\n",
    "INSTALL = True  # set False if installs are not needed (or you have no internet)\n",
    "\n",
    "if INSTALL:\n",
    "    try:\n",
    "        # Torch + torchvision for models and transforms\n",
    "        import torch, torchvision\n",
    "    except Exception:\n",
    "        %pip -q install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "    try:\n",
    "        import open_clip\n",
    "    except Exception:\n",
    "        %pip -q install open_clip_torch\n",
    "\n",
    "    try:\n",
    "        import transformers\n",
    "    except Exception:\n",
    "        %pip -q install transformers pillow\n",
    "\n",
    "# Imports\n",
    "import os, math, json, time, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# Try imports guardedly\n",
    "try:\n",
    "    import open_clip\n",
    "except Exception as e:\n",
    "    open_clip = None\n",
    "\n",
    "try:\n",
    "    from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "except Exception:\n",
    "    BlipProcessor = BlipForConditionalGeneration = None\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d4a2d",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321382d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class CFG:\n",
    "    # === Generator choices ===\n",
    "    # 'coca' (via open_clip) or 'blip' (fallback). If 'coca' fails, code auto-falls back to 'blip'.\n",
    "    GENERATOR_BACKEND = \"coca\"\n",
    "    N_CANDIDATES = 8              # number of candidates per image\n",
    "    MAX_LEN = 32                  # max tokens for generated caption\n",
    "    TOP_P = 0.9                   # nucleus sampling (used by BLIP; CoCa if supported)\n",
    "    TEMPERATURE = 1.0\n",
    "\n",
    "    # === Scoring ===\n",
    "    ALPHA = 2.0                   # weight for CLIPScore in hybrid score\n",
    "\n",
    "    # === Models ===\n",
    "    # CLIP for scoring\n",
    "    CLIP_ARCH = \"ViT-B-32\"\n",
    "    CLIP_PRETRAINED = \"openai\"\n",
    "\n",
    "    # CoCa variant (if available via open_clip)\n",
    "    COCA_ARCH = \"coca_ViT-L-14\"\n",
    "    COCA_PRETRAINED = \"mscoco_finetuned_laion2b_s13b_b90k\"\n",
    "\n",
    "    # BLIP HF id (fallback generator)\n",
    "    BLIP_MODEL_ID = \"Salesforce/blip-image-captioning-base\"\n",
    "\n",
    "CFG = CFG()\n",
    "CFG.__dict__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c7eb5",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f08f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "# A standard CLIP preprocessing pipeline (open_clip provides transforms)\n",
    "def get_clip_preprocess(clip_preprocess):\n",
    "    # open_clip returns a transform; if missing, provide a basic one\n",
    "    if clip_preprocess is not None:\n",
    "        return clip_preprocess\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                             std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac182851",
   "metadata": {},
   "source": [
    "## Load CLIP (for scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c17c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP: ViT-B-32 openai\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_clip(arch: str, pretrained: str):\n",
    "    if open_clip is None:\n",
    "        raise RuntimeError(\"open_clip not available; cannot load CLIP.\")\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(arch, pretrained=pretrained, device=device)\n",
    "    tokenizer = open_clip.get_tokenizer(arch if 'coca_' not in arch else 'ViT-L-14')  # tokenizer not used for scoring\n",
    "    model.eval()\n",
    "    return model, preprocess\n",
    "\n",
    "clip_model, clip_preprocess = load_clip(CFG.CLIP_ARCH, CFG.CLIP_PRETRAINED)\n",
    "clip_preprocess = get_clip_preprocess(clip_preprocess)\n",
    "print(\"Loaded CLIP:\", CFG.CLIP_ARCH, CFG.CLIP_PRETRAINED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0837f63",
   "metadata": {},
   "source": [
    "## CLIPScore (cosine similarity of image & text embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53df4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def clipscore(model, preprocess, image: Image.Image, captions: List[str]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    # Image -> embedding\n",
    "    img_t = preprocess(image).unsqueeze(0).to(device)\n",
    "    img_feat = model.encode_image(img_t)\n",
    "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Text -> embeddings\n",
    "    # open_clip tokenization requires the architecture name; however, we can use the generic tokenizer by model\n",
    "    # Use open_clip tokenizer via create_model_and_transforms arch inference\n",
    "    tok = open_clip.tokenize(captions).to(device)\n",
    "    txt_feat = model.encode_text(tok)\n",
    "    txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Similarities\n",
    "    sims = (img_feat @ txt_feat.T).squeeze(0)  # (N,)\n",
    "    return sims, img_feat, txt_feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd144d",
   "metadata": {},
   "source": [
    "## Candidate Generation via CoCa (preferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941e8b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Model configuration not found, returning default SimpleTokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CoCa: coca_ViT-L-14 mscoco_finetuned_laion2b_s13b_b90k\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def try_load_coca(arch: str, pretrained: str):\n",
    "    if open_clip is None:\n",
    "        return None, None, None\n",
    "    try:\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(arch, pretrained=pretrained, device=device)\n",
    "        # open_clip provides a tokenizer for CoCa text decoder\n",
    "        tok = open_clip.get_tokenizer(\"coca\") if hasattr(open_clip, \"get_tokenizer\") else None\n",
    "        model.eval()\n",
    "        return model, preprocess, tok\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Failed to load CoCa:\", e)\n",
    "        return None, None, None\n",
    "\n",
    "coca_model, coca_preprocess, coca_tokenizer = try_load_coca(CFG.COCA_ARCH, CFG.COCA_PRETRAINED)\n",
    "if coca_model:\n",
    "    print(\"Loaded CoCa:\", CFG.COCA_ARCH, CFG.COCA_PRETRAINED)\n",
    "else:\n",
    "    print(\"CoCa NOT available. Will fallback to BLIP if requested.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "467ab416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_coca(\n",
    "    model, preprocess, image: Image.Image, n_candidates: int = 8, max_len: int = 32,\n",
    "    top_p: float = 0.9, temperature: float = 1.0\n",
    ") -> Tuple[List[str], torch.Tensor]:\n",
    "    \"\"\"Generate N candidates and return (captions, log_probs).\n",
    "    We attempt to use open_clip's generation if available. If not, we raise.\n",
    "    log_probs is a (N,) tensor with approximate sequence log-likelihoods.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        raise RuntimeError(\"CoCa model not loaded.\")\n",
    "\n",
    "    # open_clip CoCa expose .generate in recent versions. If missing, raise to fallback.\n",
    "    if not hasattr(model, \"generate\"):\n",
    "        raise RuntimeError(\"This open_clip CoCa build has no `.generate`.\")\n",
    "\n",
    "    image_t = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Top-p sampling, multiple sequences\n",
    "    out = model.generate(\n",
    "        image=image_t,\n",
    "        text=None,\n",
    "        num_beams=None,\n",
    "        num_return_sequences=n_candidates,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_len=max_len,\n",
    "    )\n",
    "\n",
    "    # The generate API returns dict-like in newer builds; otherwise raw tokens.\n",
    "    if isinstance(out, dict):\n",
    "        tokens = out.get(\"sequences\")\n",
    "        logprobs = out.get(\"sequence_logprobs\", None)\n",
    "    else:\n",
    "        tokens, logprobs = out, None\n",
    "\n",
    "    # Decode\n",
    "    if hasattr(open_clip, \"decode\"):\n",
    "        captions = [open_clip.decode(t) if isinstance(t, torch.Tensor) else str(t) for t in tokens]\n",
    "    else:\n",
    "        # As a last resort, try to map tokens->string; this path is rare.\n",
    "        captions = [str(t) for t in tokens]\n",
    "\n",
    "    # Approximate log-likelihoods; if not provided, compute a proxy using CLIPScore-only later\n",
    "    if logprobs is None:\n",
    "        logprobs = torch.zeros(len(captions), device=device)\n",
    "\n",
    "    if isinstance(logprobs, list):\n",
    "        logprobs = torch.tensor(logprobs, device=device, dtype=torch.float32)\n",
    "    return captions, logprobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f8675",
   "metadata": {},
   "source": [
    "## Candidate Generation via BLIP (fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb0cad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP ready? True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def try_load_blip(model_id: str):\n",
    "    if BlipProcessor is None or BlipForConditionalGeneration is None:\n",
    "        return None, None\n",
    "    try:\n",
    "        processor = BlipProcessor.from_pretrained(model_id)\n",
    "        model = BlipForConditionalGeneration.from_pretrained(model_id).to(device)\n",
    "        model.eval()\n",
    "        return processor, model\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] Failed to load BLIP:\", e)\n",
    "        return None, None\n",
    "\n",
    "blip_processor, blip_model = try_load_blip(CFG.BLIP_MODEL_ID)\n",
    "print(\"BLIP ready?\" , blip_model is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "691ed532",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_blip(\n",
    "    processor, model, image: Image.Image, n_candidates: int = 8, max_len: int = 32,\n",
    "    top_p: float = 0.9, temperature: float = 1.0\n",
    ") -> Tuple[List[str], torch.Tensor]:\n",
    "    if processor is None or model is None:\n",
    "        raise RuntimeError(\"BLIP not loaded.\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    # N candidates via nucleus sampling\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_len,\n",
    "        num_return_sequences=n_candidates,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "    # Decode to strings\n",
    "    captions = processor.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "    # Approximate log-likelihood: sum of logits' log-probs from `scores` list\n",
    "    # Convert scores (per step) and next_tokens to logprobs\n",
    "    seq_logprobs = []\n",
    "    for i in range(len(captions)):\n",
    "        # For each returned sequence i, collect per-step token log-probs\n",
    "        # HuggingFace returns scores shared across sequences; using next_tokens per sequence is complex.\n",
    "        # Instead, we approximate by using sequence length as a weak proxy if per-token info isn't easy to extract.\n",
    "        # To keep things simple and robust, we'll compute a uniform proxy:\n",
    "        seq_logprobs.append(-len(captions[i]) / 10.0)  # proxy: mildly penalize longer strings\n",
    "    logprobs = torch.tensor(seq_logprobs, device=device, dtype=torch.float32)\n",
    "    return captions, logprobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c26c6",
   "metadata": {},
   "source": [
    "## Hybrid Scoring & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "987fe75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hybrid_score(loglik: torch.Tensor, clips: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "    # Normalize to similar ranges before combining (optional but helps stability)\n",
    "    # We z-score both arrays independently to avoid scale domination.\n",
    "    def z(x):\n",
    "        mu = x.mean()\n",
    "        sd = x.std().clamp_min(1e-6)\n",
    "        return (x - mu) / sd\n",
    "    ll_z = z(loglik)\n",
    "    cs_z = z(clips)\n",
    "    return ll_z + alpha * cs_z\n",
    "\n",
    "def select_best(captions: List[str], scores: torch.Tensor) -> Tuple[str, int]:\n",
    "    idx = int(scores.argmax().item())\n",
    "    return captions[idx], idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef117e",
   "metadata": {},
   "source": [
    "## End-to-end: One Image â†’ Best Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cc10367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def caption_image(\n",
    "    image_path: str,\n",
    "    generator_backend: str = None,\n",
    "    n_candidates: int = None,\n",
    "    max_len: int = None,\n",
    "    top_p: float = None,\n",
    "    temperature: float = None,\n",
    "    alpha: float = None\n",
    ") -> Dict:\n",
    "    t0 = time.time()\n",
    "    generator_backend = generator_backend or CFG.GENERATOR_BACKEND\n",
    "    n_candidates = n_candidates or CFG.N_CANDIDATES\n",
    "    max_len = max_len or CFG.MAX_LEN\n",
    "    top_p = top_p or CFG.TOP_P\n",
    "    temperature = temperature or CFG.TEMPERATURE\n",
    "    alpha = alpha or CFG.ALPHA\n",
    "\n",
    "    img = load_image(image_path)\n",
    "\n",
    "    captions, loglik = [], None\n",
    "    used_backend = None\n",
    "    # Try CoCa first if requested\n",
    "    if generator_backend == \"coca\":\n",
    "        try:\n",
    "            caps, ll = generate_with_coca(coca_model, coca_preprocess or clip_preprocess, img,\n",
    "                                          n_candidates=n_candidates, max_len=max_len, top_p=top_p, temperature=temperature)\n",
    "            captions, loglik = caps, ll\n",
    "            used_backend = \"coca\"\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] CoCa generation failed:\", e)\n",
    "\n",
    "    # Fallback to BLIP\n",
    "    if (not captions) and (blip_model is not None):\n",
    "        caps, ll = generate_with_blip(blip_processor, blip_model, img,\n",
    "                                      n_candidates=n_candidates, max_len=max_len, top_p=top_p, temperature=temperature)\n",
    "        captions, loglik = caps, ll\n",
    "        used_backend = \"blip\"\n",
    "\n",
    "    if not captions:\n",
    "        raise RuntimeError(\"No generator available. Install/enable CoCa or BLIP.\")\n",
    "\n",
    "    # CLIPScore\n",
    "    clips, _, _ = clipscore(clip_model, clip_preprocess, img, captions)\n",
    "\n",
    "    # Hybrid\n",
    "    scores = hybrid_score(loglik, clips, alpha)\n",
    "\n",
    "    best_caption, best_idx = select_best(captions, scores)\n",
    "    elapsed = time.time() - t0\n",
    "    return {\n",
    "        \"image\": image_path,\n",
    "        \"backend\": used_backend,\n",
    "        \"captions\": captions,\n",
    "        \"loglik\": [float(x) for x in loglik.cpu()],\n",
    "        \"clips\": [float(x) for x in clips.cpu()],\n",
    "        \"hybrid\": [float(x) for x in scores.cpu()],\n",
    "        \"best_caption\": best_caption,\n",
    "        \"best_idx\": int(best_idx),\n",
    "        \"time_s\": elapsed,\n",
    "    }\n",
    "\n",
    "# Pretty print helper\n",
    "def print_result(res: Dict):\n",
    "    print(f\"Image: {res['image']}\")\n",
    "    print(f\"Backend: {res['backend']}  |  time: {res['time_s']:.2f}s\")\n",
    "    print(\"Top choice â†’\", res[\"best_caption\"])\n",
    "    print(\"--- Candidates (cap / loglik / clip / hybrid) ---\")\n",
    "    for i, (c, ll, cs, hy) in enumerate(zip(res[\"captions\"], res[\"loglik\"], res[\"clips\"], res[\"hybrid\"])):\n",
    "        tag = \"  <-- BEST\" if i == res[\"best_idx\"] else \"\"\n",
    "        print(f\"[{i:02d}] {c} | ll={ll:+.3f} | clip={cs:+.3f} | hybrid={hy:+.3f}{tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ed0a3",
   "metadata": {},
   "source": [
    "## Batch Inference over a Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc1a37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def caption_folder(\n",
    "    folder: str,\n",
    "    exts: Tuple[str, ...] = (\".jpg\", \".jpeg\", \".png\"),\n",
    "    limit: Optional[int] = None,\n",
    "    **kwargs\n",
    ") -> pd.DataFrame:\n",
    "    paths = [str(p) for p in Path(folder).glob(\"**/*\") if p.suffix.lower() in exts]\n",
    "    if limit:\n",
    "        paths = paths[:limit]\n",
    "    rows = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            res = caption_image(p, **kwargs)\n",
    "            rows.append({\n",
    "                \"image\": p,\n",
    "                \"backend\": res[\"backend\"],\n",
    "                \"best_caption\": res[\"best_caption\"],\n",
    "                \"best_idx\": res[\"best_idx\"],\n",
    "                \"time_s\": res[\"time_s\"],\n",
    "                \"candidates\": json.dumps(res[\"captions\"], ensure_ascii=False),\n",
    "                \"loglik\": json.dumps(res[\"loglik\"]),\n",
    "                \"clips\": json.dumps(res[\"clips\"]),\n",
    "                \"hybrid\": json.dumps(res[\"hybrid\"]),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            rows.append({\"image\": p, \"backend\": \"error\", \"best_caption\": str(e), \"best_idx\": -1, \"time_s\": -1, \"candidates\": \"[]\", \"loglik\": \"[]\", \"clips\": \"[]\", \"hybrid\": \"[]\"})\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Example:\n",
    "# df = caption_folder(\"/path/to/images\", limit=5, generator_backend=\"coca\")\n",
    "# df.to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb47f64",
   "metadata": {},
   "source": [
    "## Provide Your Own Candidates (Rerank-only mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6fd59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rerank_only(image_path: str, candidates: List[str], alpha: float = None) -> Dict:\n",
    "    alpha = alpha or CFG.ALPHA\n",
    "    img = load_image(image_path)\n",
    "    clips, _, _ = clipscore(clip_model, clip_preprocess, img, candidates)\n",
    "    # Use zero log-likelihoods if you don't have them\n",
    "    loglik = torch.zeros(len(candidates), device=device)\n",
    "    scores = hybrid_score(loglik, clips, alpha)\n",
    "    best_caption, best_idx = select_best(candidates, scores)\n",
    "    return {\n",
    "        \"image\": image_path,\n",
    "        \"backend\": \"rerank_only\",\n",
    "        \"captions\": candidates,\n",
    "        \"loglik\": [0.0] * len(candidates),\n",
    "        \"clips\": [float(x) for x in clips.cpu()],\n",
    "        \"hybrid\": [float(x) for x in scores.cpu()],\n",
    "        \"best_caption\": best_caption,\n",
    "        \"best_idx\": int(best_idx),\n",
    "        \"time_s\": 0.0,\n",
    "    }\n",
    "\n",
    "# Example:\n",
    "# res = rerank_only(\"example.jpg\", [\"a dog in grass\", \"a cat on sofa\", \"a brown dog running\"])\n",
    "# print_result(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7cfbb2",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93960d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Set DEMO_IMAGE to a valid image path and re-run.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ðŸ”§ Set your image path here and run this cell.\n",
    "DEMO_IMAGE = \"/path/to/your/image.jpg\"   # <-- change me\n",
    "if os.path.exists(DEMO_IMAGE):\n",
    "    out = caption_image(DEMO_IMAGE, generator_backend=CFG.GENERATOR_BACKEND)\n",
    "    print_result(out)\n",
    "else:\n",
    "    print(\"[Info] Set DEMO_IMAGE to a valid image path and re-run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f08d3a",
   "metadata": {},
   "source": [
    "## Save Results Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b966f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_json(obj, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Saved:\", path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
