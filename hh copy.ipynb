{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoCaps Validation â€” OpenCLIP CoCa End-to-End (Option A)\n",
    "\n",
    "This notebook:\n",
    "1. Installs dependencies (OpenCLIP CoCa + COCO caption eval).\n",
    "2. Loads NoCaps **validation** annotations (expects 10 captions per image).\n",
    "3. Loads a **pretrained CoCa** from OpenCLIP.\n",
    "4. Generates captions (beam search) for all images.\n",
    "5. Evaluates with **BLEU, METEOR, ROUGE_L, CIDEr** (SPICE optional).\n",
    "\n",
    "> **Paths to set:** `ANN_PATH` and `IMG_DIR` near the top.  \n",
    "> **Expected files:**  \n",
    ">  - `data/nocap_val_4500_captions.json`  \n",
    ">  - `data/validation/<image files>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.12/site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: open_clip_torch in ./venv/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.12/site-packages (11.3.0)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.23.0)\n",
      "Requirement already satisfied: pycocotools in ./venv/lib/python3.12/site-packages (2.0.10)\n",
      "Requirement already satisfied: torch>=2.0 in ./venv/lib/python3.12/site-packages (from open_clip_torch) (2.8.0)\n",
      "Requirement already satisfied: regex in ./venv/lib/python3.12/site-packages (from open_clip_torch) (2025.9.18)\n",
      "Requirement already satisfied: ftfy in ./venv/lib/python3.12/site-packages (from open_clip_torch) (6.3.1)\n",
      "Requirement already satisfied: huggingface-hub in ./venv/lib/python3.12/site-packages (from open_clip_torch) (0.35.1)\n",
      "Requirement already satisfied: safetensors in ./venv/lib/python3.12/site-packages (from open_clip_torch) (0.6.2)\n",
      "Requirement already satisfied: timm>=1.0.17 in ./venv/lib/python3.12/site-packages (from open_clip_torch) (1.0.20)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.12/site-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from ftfy->open_clip_torch) (0.2.14)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (1.1.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
      "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-l9sqn96x\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-l9sqn96x\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# --- Install dependencies (internet required) ---\n",
    "%pip install --upgrade pip\n",
    "%pip install open_clip_torch pillow tqdm torchvision pycocotools\n",
    "%pip install git+https://github.com/salaniz/pycocoevalcap\n",
    "# Optional for SPICE (Java required):\n",
    "# !apt-get update && apt-get install -y default-jre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import open_clip\n",
    "from torchvision import transforms\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "# ---- Set your paths here ----\n",
    "ANN_PATH = \"data/nocap_val_4500_captions.json\"   # NoCaps validation annotations\n",
    "IMG_DIR  = \"data/validation\"                      # folder containing validation images\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "assert Path(ANN_PATH).exists(), f\"Annotation file not found: {ANN_PATH}\"\n",
    "assert Path(IMG_DIR).exists(), f\"Image folder not found: {IMG_DIR}\"\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load annotations + verify 10 refs per image ---\n",
    "with open(ANN_PATH, \"r\") as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "id2file = {img[\"id\"]: img[\"file_name\"] for img in ann[\"images\"]}\n",
    "caps_by_id = defaultdict(list)\n",
    "for a in ann[\"annotations\"]:\n",
    "    caps_by_id[a[\"image_id\"]].append(a[\"caption\"])\n",
    "\n",
    "num_images = len(ann[\"images\"])\n",
    "lens = [len(caps_by_id[i[\"id\"]]) for i in ann[\"images\"]]\n",
    "\n",
    "print(f\"# images: {num_images}\")\n",
    "print(f\"min refs: {min(lens)}, max refs: {max(lens)}, mean refs: {sum(lens)/len(lens):.2f}\")\n",
    "\n",
    "bad = [(i[\"id\"], id2file[i[\"id\"]], len(caps_by_id[i[\"id\"]])) for i in ann[\"images\"] if len(caps_by_id[i[\"id\"]]) != 10]\n",
    "print(\"non-10 reference counts:\", len(bad))\n",
    "if not bad:\n",
    "    first = ann[\"images\"][0]\n",
    "    print(\"Example image:\", first[\"file_name\"])\n",
    "    print(\"Refs:\", caps_by_id[first[\"id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae616ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load OpenCLIP CoCa ---\n",
    "\n",
    "''' \n",
    "('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
    "('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
    "('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
    "('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
    "'''\n",
    "\n",
    "model_name = \"coca_ViT-L-14\"\n",
    "pretrained_tag = \"mscoco_finetuned_laion2b_s13b_b90k\"\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained_tag)\n",
    "tokenizer = open_clip.get_tokenizer(model_name)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "print(\"Loaded:\", model_name, \"/\", pretrained_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    from open_clip import tokenizer as openclip_tok_mod\n",
    "    _have_openclip_decoder = hasattr(openclip_tok_mod, \"decode\")\n",
    "except Exception:\n",
    "    openclip_tok_mod = None\n",
    "    _have_openclip_decoder = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption_openclip(pil_img, max_len=30, temperature=1.0, top_k=None, top_p=None):\n",
    "    img = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    tried = []\n",
    "    out = None\n",
    "    for kwargs in (\n",
    "        dict(seq_len=max_len, temperature=temperature, top_k=top_k, top_p=top_p),\n",
    "        dict(seq_len=max_len, temperature=temperature),\n",
    "        dict(max_len=max_len, temperature=temperature),\n",
    "        dict(seq_len=max_len),\n",
    "        dict(max_len=max_len),\n",
    "        dict(),\n",
    "    ):\n",
    "        try:\n",
    "            out = model.generate(img, **{k: v for k, v in kwargs.items() if v is not None})\n",
    "            break\n",
    "        except TypeError as e:\n",
    "            tried.append(str(e))\n",
    "            out = None\n",
    "\n",
    "    if out is None:\n",
    "        raise RuntimeError(\"open_clip CoCa.generate() signature not recognized. Tried:\\n\" + \"\\n\".join(tried))\n",
    "\n",
    "    # --- decode handling ---\n",
    "    if isinstance(out, list):\n",
    "        if len(out) and isinstance(out[0], str):\n",
    "            return out[0]\n",
    "        if len(out) and torch.is_tensor(out[0]):\n",
    "            ids = out[0]\n",
    "        elif len(out) and isinstance(out[0], (list, tuple)):\n",
    "            ids = torch.tensor(out[0])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unexpected list return type from model.generate(): {type(out[0])}\")\n",
    "    elif torch.is_tensor(out):\n",
    "        ids = out[0]\n",
    "    else:\n",
    "        return str(out)\n",
    "\n",
    "    if not torch.is_tensor(ids):\n",
    "        ids = torch.tensor(ids)\n",
    "    if _have_openclip_decoder:\n",
    "        return openclip_tok_mod.decode(ids)\n",
    "    if hasattr(model, \"tokenizer\") and hasattr(model.tokenizer, \"decode\"):\n",
    "        return model.tokenizer.decode(ids.tolist())\n",
    "\n",
    "    raise RuntimeError(\"model.generate returned token IDs but no decoder is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = Path(IMG_DIR) / id2file[ann[\"images\"][0][\"id\"]]\n",
    "print(\"Test caption:\", generate_caption_openclip(Image.open(test_path).convert(\"RGB\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51485ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    raw = model.generate(preprocess(Image.open(test_path).convert(\"RGB\")).unsqueeze(0).to(device), seq_len=30)\n",
    "print(type(raw), isinstance(raw, list), torch.is_tensor(raw))\n",
    "print(raw[:1] if isinstance(raw, list) else raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate predictions for all images ---\n",
    "preds, missing = [], []\n",
    "for img_info in tqdm(ann[\"images\"], desc=\"Captioning\"):\n",
    "    iid, fname = img_info[\"id\"], img_info[\"file_name\"]\n",
    "    path = Path(IMG_DIR) / fname\n",
    "    if not path.exists():\n",
    "        missing.append(fname); continue\n",
    "    pil = Image.open(path).convert(\"RGB\")\n",
    "    cap = generate_caption_openclip(pil)\n",
    "    preds.append({\"image_id\": iid, \"caption\": cap})\n",
    "\n",
    "print(\"Generated:\", len(preds), \"/\", num_images)\n",
    "print(\"Missing images:\", len(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save predictions ---\n",
    "OUT_JSON = \"preds_nocaps_val_openclip.json\"\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(preds, f)\n",
    "print(\"Saved:\", OUT_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "# Load GT and predictions\n",
    "coco = COCO(ANN_PATH)\n",
    "cocoRes = coco.loadRes(OUT_JSON)\n",
    "\n",
    "evaluator = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "# Replace the default scorers (which includes SPICE)\n",
    "evaluator.scorers = [\n",
    "    (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "    (Meteor(), \"METEOR\"),\n",
    "    (Rouge(), \"ROUGE_L\"),\n",
    "    (Cider(), \"CIDEr\"),\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluator.evaluate()\n",
    "\n",
    "print(\"\\n=== NoCaps-val (overall, no SPICE) ===\")\n",
    "for k, v in evaluator.eval.items():\n",
    "    print(f\"{k:10s}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Show some qualitative examples ---\n",
    "import random\n",
    "from IPython.display import display\n",
    "\n",
    "def show_examples(n=3):\n",
    "    ids = [im[\"id\"] for im in random.sample(ann[\"images\"], k=n)]\n",
    "    for iid in ids:\n",
    "        f = id2file[iid]\n",
    "        p = Path(IMG_DIR) / f\n",
    "        if not p.exists():\n",
    "            print(\"Missing:\", f); continue\n",
    "        img = Image.open(p).convert(\"RGB\").resize((384, 384))\n",
    "        display(img)\n",
    "        ref_caps = caps_by_id[iid][:3]\n",
    "        gen = next((x[\"caption\"] for x in preds if x[\"image_id\"] == iid), None)\n",
    "        print(\"Generated:\", iid,gen)\n",
    "        print(\"Refs:\")\n",
    "        for rc in ref_caps:\n",
    "            print(\"  -\", rc)\n",
    "        print(\"-\"*80)\n",
    "\n",
    "show_examples(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Beam size 3â€“5 is good for CIDEr.\n",
    "- Max caption length ~20â€“30 tokens.\n",
    "- SPICE metric requires Java.\n",
    "- Leaderboard results differ (use online eval server for test split)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
