{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d471d64e",
   "metadata": {},
   "source": [
    "\n",
    "# NoCaps Validation — CoCa Generation + CLIP-Guided Reranking (Option A)\n",
    "\n",
    "This notebook:\n",
    "1. Loads **NoCaps validation** annotations.\n",
    "2. Loads **OpenCLIP CoCa** for caption generation (one or many candidates).\n",
    "3. Loads **OpenAI CLIP (ViT-B/32)** for scoring (CLIPScore = cosine similarity).\n",
    "4. Generates **N candidates per image** (via repeated generation calls with temperature).\n",
    "5. **Reranks** the N candidates by CLIPScore and keeps the best per image.\n",
    "6. Evaluates with **BLEU, METEOR, ROUGE_L, CIDEr** *(SPICE skipped to avoid Java)*.\n",
    "7. Shows qualitative examples.\n",
    "\n",
    "> Set your paths in the **Config** cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfba1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install dependencies (internet required) ---\n",
    "%pip install --upgrade pip\n",
    "%pip install open_clip_torch pillow tqdm torchvision pycocotools\n",
    "%pip install git+https://github.com/salaniz/pycocoevalcap\n",
    "# SPICE is skipped by design; if you later want SPICE, install Java 11 and do not skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7094f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Imports & Config ---\n",
    "import os, json, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import open_clip\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "# ====== Paths: change these to your local files ======\n",
    "ANN_PATH = \"data/nocap_val_4500_captions.json\"   # NoCaps validation annotations\n",
    "IMG_DIR  = \"data/validation\"                      # Folder with validation images\n",
    "\n",
    "# Generation & reranking hyperparams\n",
    "N_CANDIDATES = 5       # N candidates per image (increase for stronger reranking)\n",
    "SEQ_LEN      = 28      # caption length\n",
    "TEMP         = 1.1     # >1.0 adds diversity; if outputs repeat, try 1.2~1.3\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Basic checks\n",
    "assert Path(ANN_PATH).exists(), f\"Annotation file not found: {ANN_PATH}\"\n",
    "assert Path(IMG_DIR).exists(), f\"Image folder not found: {IMG_DIR}\"\n",
    "\n",
    "# Reproducibility for any deterministic parts\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4598aeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# images: 4500\n",
      "min refs: 10, max refs: 10, mean refs: 10.00\n",
      "non-10 reference counts: 0\n",
      "Example image: 0013ea2087020901.jpg\n",
      "Refs: ['A baby is standing in front of a house.', 'A little girl in a white jacket and sandals.', 'A young child stands in front of a house.'] ...\n"
     ]
    }
   ],
   "source": [
    "# --- Load annotations + verify 10 refs per image ---\n",
    "with open(ANN_PATH, \"r\") as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "id2file = {img[\"id\"]: img[\"file_name\"] for img in ann[\"images\"]}\n",
    "caps_by_id = defaultdict(list)\n",
    "for a in ann[\"annotations\"]:\n",
    "    caps_by_id[a[\"image_id\"]].append(a[\"caption\"])\n",
    "\n",
    "num_images = len(ann[\"images\"])\n",
    "lens = [len(caps_by_id[i[\"id\"]]) for i in ann[\"images\"]]\n",
    "\n",
    "print(f\"# images: {num_images}\")\n",
    "print(f\"min refs: {min(lens)}, max refs: {max(lens)}, mean refs: {sum(lens)/len(lens):.2f}\")\n",
    "\n",
    "bad = [(i[\"id\"], id2file[i[\"id\"]], len(caps_by_id[i[\"id\"]])) for i in ann[\"images\"] if len(caps_by_id[i[\"id\"]]) != 10]\n",
    "print(\"non-10 reference counts:\", len(bad))\n",
    "if not bad:\n",
    "    ex = ann[\"images\"][0]\n",
    "    print(\"Example image:\", ex[\"file_name\"])\n",
    "    print(\"Refs:\", caps_by_id[ex[\"id\"]][:3], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1d47a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CoCa: coca_ViT-L-14 / mscoco_finetuned_laion2b_s13b_b90k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CLIP: ViT-B-32 / openai\n"
     ]
    }
   ],
   "source": [
    "# --- Load CoCa (for generation) ---\n",
    "coca_name = \"coca_ViT-L-14\"\n",
    "coca_tag  = \"mscoco_finetuned_laion2b_s13b_b90k\"  # strong for captioning\n",
    "\n",
    "coca_model, _, coca_preprocess = open_clip.create_model_and_transforms(coca_name, pretrained=coca_tag)\n",
    "coca_model = coca_model.to(device).eval()\n",
    "print(\"Loaded CoCa:\", coca_name, \"/\", coca_tag)\n",
    "\n",
    "# --- Load CLIP (for scoring) ---\n",
    "clip_name = \"ViT-B-32\"\n",
    "clip_tag  = \"openai\"\n",
    "\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(clip_name, pretrained=clip_tag)\n",
    "clip_tokenizer = open_clip.get_tokenizer(clip_name)\n",
    "clip_model = clip_model.to(device).eval()\n",
    "print(\"Loaded CLIP:\", clip_name, \"/\", clip_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95f8ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust CoCa.generate wrapper ---\n",
    "try:\n",
    "    from open_clip import tokenizer as openclip_tok_mod\n",
    "    _have_openclip_decoder = hasattr(openclip_tok_mod, \"decode\")\n",
    "except Exception:\n",
    "    openclip_tok_mod = None\n",
    "    _have_openclip_decoder = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption_openclip(pil_img, max_len=30, temperature=1.0, top_k=None, top_p=None):\n",
    "    img = coca_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    tried = []\n",
    "    out = None\n",
    "    for kwargs in (\n",
    "        dict(seq_len=max_len, temperature=temperature, top_k=top_k, top_p=top_p),\n",
    "        dict(seq_len=max_len, temperature=temperature),\n",
    "        dict(max_len=max_len, temperature=temperature),\n",
    "        dict(seq_len=max_len),\n",
    "        dict(max_len=max_len),\n",
    "        dict(),\n",
    "    ):\n",
    "        try:\n",
    "            clean = {k: v for k, v in kwargs.items() if v is not None}\n",
    "            out = coca_model.generate(img, **clean)\n",
    "            break\n",
    "        except TypeError as e:\n",
    "            tried.append(str(e))\n",
    "            out = None\n",
    "\n",
    "    if out is None:\n",
    "        raise RuntimeError(\"CoCa.generate() signature not recognized. Tried:\\\\n\" + \"\\\\n\".join(tried))\n",
    "\n",
    "    if isinstance(out, list):\n",
    "        if len(out) and isinstance(out[0], str):\n",
    "            return out[0]\n",
    "        if len(out) and torch.is_tensor(out[0]):\n",
    "            ids = out[0]\n",
    "        elif len(out) and isinstance(out[0], (list, tuple)):\n",
    "            ids = torch.tensor(out[0])\n",
    "        else:\n",
    "            return str(out)\n",
    "    elif torch.is_tensor(out):\n",
    "        ids = out[0]\n",
    "    else:\n",
    "        return str(out)\n",
    "\n",
    "    if not torch.is_tensor(ids):\n",
    "        ids = torch.tensor(ids)\n",
    "\n",
    "    if _have_openclip_decoder:\n",
    "        return openclip_tok_mod.decode(ids)\n",
    "    if hasattr(coca_model, \"tokenizer\") and hasattr(coca_model.tokenizer, \"decode\"):\n",
    "        return coca_model.tokenizer.decode(ids.tolist())\n",
    "\n",
    "    return \" \".join(map(str, ids.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca2636cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate N candidates (repeat with temperature for diversity) ---\n",
    "@torch.no_grad()\n",
    "def generate_n_candidates(pil_img, N=5, seq_len=28, temperature=1.1):\n",
    "    caps = []\n",
    "    for _ in range(N):\n",
    "        cap = generate_caption_openclip(pil_img, max_len=seq_len, temperature=temperature)\n",
    "        caps.append(cap)\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set(); uniq = []\n",
    "    for c in caps:\n",
    "        if c not in seen:\n",
    "            seen.add(c); uniq.append(c)\n",
    "    return uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9b7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLIPScore computation and reranking ---\n",
    "@torch.no_grad()\n",
    "def clipscore_rerank(pil_img, candidates):\n",
    "    if not candidates:\n",
    "        return None, []\n",
    "\n",
    "    # Encode image once\n",
    "    img = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    img_feat = clip_model.encode_image(img)\n",
    "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Encode all candidate captions\n",
    "    toks = clip_tokenizer(candidates).to(device)\n",
    "    txt_feat = clip_model.encode_text(toks)\n",
    "    txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    sims = (img_feat @ txt_feat.T).squeeze(0).detach().float().cpu().tolist()\n",
    "    ranked = sorted(zip(candidates, sims), key=lambda x: x[1], reverse=True)\n",
    "    best_cap = ranked[0][0]\n",
    "    return best_cap, ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64b65ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates:\n",
      " - <start_of_text>a garage that has been torn down on the side of the street . <end_of_text>\n",
      "\n",
      "Top-1 by CLIPScore:\n",
      " <start_of_text>a garage that has been torn down on the side of the street . <end_of_text>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Quick test on one image ---\n",
    "test_path = Path(IMG_DIR) / id2file[ann[\"images\"][8][\"id\"]]\n",
    "if test_path.exists():\n",
    "    pil = Image.open(test_path).convert(\"RGB\")\n",
    "    cands = generate_n_candidates(pil, N=N_CANDIDATES, seq_len=SEQ_LEN, temperature=TEMP)\n",
    "    best, ranked = clipscore_rerank(pil, cands)\n",
    "    print(\"Candidates:\")\n",
    "    for c in cands:\n",
    "        print(\" -\", c)\n",
    "    print(\"\\nTop-1 by CLIPScore:\\n\", best)\n",
    "else:\n",
    "    print(\"Test image missing:\", test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full dataset: generate N candidates + CLIP rerank ---\n",
    "preds = []\n",
    "missing = []\n",
    "all_candidates_debug = []   # optional: save ranked lists per image\n",
    "\n",
    "for img_info in tqdm(ann[\"images\"], desc=\"Captioning + Reranking\"):\n",
    "    iid, fname = img_info[\"id\"], img_info[\"file_name\"]\n",
    "    fpath = Path(IMG_DIR) / fname\n",
    "    if not fpath.exists():\n",
    "        missing.append(fname)\n",
    "        continue\n",
    "\n",
    "    pil = Image.open(fpath).convert(\"RGB\")\n",
    "    cands = generate_n_candidates(pil, N=N_CANDIDATES, seq_len=SEQ_LEN, temperature=TEMP)\n",
    "    if not cands:\n",
    "        cands = [generate_caption_openclip(pil, max_len=SEQ_LEN, temperature=TEMP)]\n",
    "\n",
    "    best, ranked = clipscore_rerank(pil, cands)\n",
    "    preds.append({\"image_id\": iid, \"caption\": best})\n",
    "    all_candidates_debug.append({\n",
    "        \"image_id\": iid,\n",
    "        \"file_name\": fname,\n",
    "        \"ranked\": [{\"caption\": c, \"clipscore\": s} for c, s in ranked]\n",
    "    })\n",
    "\n",
    "print(\"Generated:\", len(preds), \"/\", len(ann[\"images\"]))\n",
    "print(\"Missing images:\", len(missing))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fafb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Save outputs ---\n",
    "OUT_JSON = \"preds_nocaps_val_openclip_cliprank.json\"\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(preds, f)\n",
    "print(\"Saved predictions:\", OUT_JSON)\n",
    "\n",
    "DEBUG_JSON = \"nocaps_candidates_cliprank.json\"\n",
    "with open(DEBUG_JSON, \"w\") as f:\n",
    "    json.dump(all_candidates_debug, f, indent=2)\n",
    "print(\"Saved candidates (debug):\", DEBUG_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- COCO caption metrics (BLEU/METEOR/ROUGE_L/CIDEr), SPICE skipped ---\n",
    "coco = COCO(ANN_PATH)\n",
    "cocoRes = coco.loadRes(OUT_JSON)\n",
    "\n",
    "evaluator = COCOEvalCap(coco, cocoRes)\n",
    "# Replace default scorers to avoid SPICE (Java)\n",
    "evaluator.scorers = [\n",
    "    (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "    (Meteor(), \"METEOR\"),\n",
    "    (Rouge(), \"ROUGE_L\"),\n",
    "    (Cider(), \"CIDEr\"),\n",
    "]\n",
    "evaluator.evaluate()\n",
    "\n",
    "print(\"\\n=== NoCaps-val (overall, no SPICE) ===\")\n",
    "for k, v in evaluator.eval.items():\n",
    "    print(f\"{k:10s}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087aba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Qualitative examples: show a few images, top caption, and references ---\n",
    "import random\n",
    "from IPython.display import display\n",
    "\n",
    "def show_examples(n=3):\n",
    "    ids = [im[\"id\"] for im in random.sample(ann[\"images\"], k=n)]\n",
    "    for iid in ids:\n",
    "        fname = id2file[iid]\n",
    "        p = Path(IMG_DIR) / fname\n",
    "        if not p.exists():\n",
    "            print(\"Missing:\", fname); continue\n",
    "        display(Image.open(p).convert(\"RGB\"))\n",
    "        gen = next((x[\"caption\"] for x in preds if x[\"image_id\"] == iid), None)\n",
    "        print(\"Top-1 (CLIP rerank):\", gen)\n",
    "        print(\"Refs:\")\n",
    "        for rc in caps_by_id[iid][:3]:\n",
    "            print(\"  -\", rc)\n",
    "        print(\"-\"*80)\n",
    "\n",
    "show_examples(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ccc8d0",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Tips\n",
    "- **Transforms**: `coca_preprocess` for CoCa; `clip_preprocess` for CLIP.\n",
    "- **Diversity**: If candidates repeat, increase `TEMP` to 1.2–1.3. If still identical, your OpenCLIP build may be deterministic; upgrade OpenCLIP or implement a custom decoder.\n",
    "- **N candidates**: 5–10 is a good range; higher N improves reranking but costs time.\n",
    "- **SPICE**: Skipped here to avoid Java. If needed, install Java 11 and revert to default scorers.\n",
    "- **Speed**: CLIP ViT-B/32 is fast; CoCa L-14 is heavy. For quick trials, try `coca_ViT-B-32`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
