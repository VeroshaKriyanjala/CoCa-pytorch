{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903f1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "images = [os.path.join('data/images', fname) for fname in os.listdir('data/images')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80532ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [os.path.splitext(os.path.basename(img))[0] for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a30e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('data/nocap_val_4500_captions.json', 'r') as f:\n",
    "    captions_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results = []\n",
    "for img in captions_data['images']:\n",
    "    image_name = os.path.splitext(img['file_name'])[0]\n",
    "    image_id = img['id']\n",
    "    # Get all captions for this image_id\n",
    "    image_captions = [\n",
    "        {'captionid': cap['id'], 'caption': cap['caption']}\n",
    "        for cap in captions_data['annotations'] if cap['image_id'] == image_id\n",
    "    ]\n",
    "    grouped_results.append({\n",
    "        'image_name': image_name,\n",
    "        'image_id': image_id,\n",
    "        'captions': image_captions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1841e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e747987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_path = 'data/validation/' + grouped_results[0]['image_name'] + '.jpg'\n",
    "img = Image.open(img_path)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# import vision transformer\n",
    "\n",
    "from vit_pytorch.simple_vit_with_patch_dropout import SimpleViT\n",
    "from vit_pytorch.extractor import Extractor\n",
    "\n",
    "vit = SimpleViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    patch_dropout = 0.5  # https://arxiv.org/abs/2212.00794\n",
    ")\n",
    "\n",
    "vit = Extractor(vit, return_embeddings_only = True, detach = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coca_pytorch.coca_pytorch import CoCa\n",
    "\n",
    "coca = CoCa(\n",
    "    dim = 512,                     # model dimension\n",
    "    img_encoder = vit,             # vision transformer - image encoder, returning image embeddings as (batch, seq, dim)\n",
    "    image_dim = 1024,              # image embedding dimension, if not the same as model dimensions\n",
    "    num_tokens = 20000,            # number of text tokens\n",
    "    unimodal_depth = 6,            # depth of the unimodal transformer\n",
    "    multimodal_depth = 6,          # depth of the multimodal transformer\n",
    "    dim_head = 64,                 # dimension per attention head\n",
    "    heads = 8,                     # number of attention heads\n",
    "    caption_loss_weight = 1.,      # weight on the autoregressive caption loss\n",
    "    contrastive_loss_weight = 1.,  # weight on the contrastive loss between image and text CLS embeddings\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb774847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, collections\n",
    "\n",
    "ANN = 'data/nocap_val_4500_captions.json'\n",
    "IMG_DIR = 'data/validation'  # where you downloaded images\n",
    "\n",
    "with open(ANN, 'r') as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "# maps\n",
    "id2file = {img['id']: img['file_name'] for img in ann['images']}\n",
    "file2id = {v:k for k,v in id2file.items()}\n",
    "\n",
    "# group refs by image_id\n",
    "caps_by_id = collections.defaultdict(list)\n",
    "for a in ann['annotations']:\n",
    "    caps_by_id[a['image_id']].append(a['caption'])\n",
    "\n",
    "num_images = len(ann['images'])\n",
    "lens = [len(caps_by_id[i['id']]) for i in ann['images']]\n",
    "print(f\"#images: {num_images}, min refs: {min(lens)}, max refs: {max(lens)}, mean refs: {sum(lens)/len(lens):.2f}\")\n",
    "\n",
    "# sanity: list any images not having exactly 10 refs\n",
    "bad = [(i['id'], id2file[i['id']], len(caps_by_id[i['id']])) for i in ann['images'] if len(caps_by_id[i['id']]) != 10]\n",
    "print(\"non-10 reference counts:\", len(bad))\n",
    "bad[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_image(path):\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "def generate_caption(model, pil_img):\n",
    "    # TODO: YOUR decoding here (greedy/beam). Return a plain string.\n",
    "    # Example placeholder:\n",
    "    return \"a placeholder caption for this image\"\n",
    "\n",
    "# Produce predictions for ALL images in the JSON file order\n",
    "preds = []\n",
    "for img in tqdm(ann['images']):\n",
    "    image_id = img['id']\n",
    "    img_path = os.path.join(IMG_DIR, img['file_name'])\n",
    "    pil_img = load_image(img_path)\n",
    "\n",
    "    caption = generate_caption(model=None, pil_img=pil_img)  # replace model=None with your CoCa\n",
    "    preds.append({\"image_id\": image_id, \"caption\": caption})\n",
    "\n",
    "len(preds), preds[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa669ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import json, tempfile\n",
    "\n",
    "# build a “results” json file from preds\n",
    "tmp_results = tempfile.NamedTemporaryFile(suffix=\".json\", delete=False).name\n",
    "with open(tmp_results, \"w\") as f:\n",
    "    json.dump(preds, f)\n",
    "\n",
    "coco = COCO(ANN)\n",
    "cocoRes = coco.loadRes(tmp_results)\n",
    "\n",
    "evaluator = COCOEvalCap(coco, cocoRes)\n",
    "evaluator.evaluate()\n",
    "\n",
    "# Print key metrics\n",
    "for k, v in evaluator.eval.items():\n",
    "    print(f\"{k:8s}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f061c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "''' \n",
    "('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
    " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
    " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
    " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
    " '''\n",
    "\n",
    "model, _, transform = open_clip.create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-L-14\",\n",
    "  pretrained=\"mscoco_finetuned_laion2b_s13b_b90k\"\n",
    ")\n",
    "\n",
    "im = Image.open(\"Coca/images/golden.jpeg\").convert(\"RGB\")\n",
    "im = transform(im).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "  generated = model.generate(im)\n",
    "\n",
    "print(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc5250",
   "metadata": {},
   "source": [
    "1 a picture of one of the golden retreiver on the beach . one of the golden retreiver on the beach is looking at the camera\n",
    "2 a brown dog is sitting on the beach\n",
    "3 dog friendly beaches in sydney\n",
    "4 a dog sitting on top of a sandy beach next to the ocean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
