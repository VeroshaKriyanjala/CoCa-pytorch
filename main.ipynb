{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cb10ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse_g3/LiteGAD/CoCa-pytorch/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cse_g3/LiteGAD/CoCa-pytorch/.venv/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "import open_clip\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- CoCa bits (replace with your integration) ----\n",
    "# Assume you have:\n",
    "#   coca.generate(image) -> List[ { \"tokens\": LongTensor, \"logprob\": float } ]  # K candidates\n",
    "#   coca.tokenizer.decode(ids) -> str\n",
    "#   coca.preprocess(Image) -> Tensor (B, C, H, W)\n",
    "\n",
    "# ---- OpenCLIP for CLIPScore ----\n",
    "clip_model, clip_preprocess, clip_tokenizer = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"openai\"\n",
    ")\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_image_embed(pil_img):\n",
    "    t = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    img_feat = clip_model.encode_image(t)\n",
    "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "    return img_feat.squeeze(0)  # (D,)\n",
    "\n",
    "@torch.no_grad()\n",
    "def clip_text_embed(texts):\n",
    "    tok = clip_tokenizer(texts).to(device)\n",
    "    txt_feat = clip_model.encode_text(tok)\n",
    "    txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "    return txt_feat  # (B, D)\n",
    "\n",
    "def clipscore(img_feat, captions):\n",
    "    txt_feat = clip_text_embed(captions)     # (K, D)\n",
    "    sims = (txt_feat @ img_feat.unsqueeze(1)).squeeze(1)  # (K,)\n",
    "    return sims  # cosine similarity in [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fcd6490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_normalize(logprob, length, length_penalty=0.7):\n",
    "    # same spirit as GNMT: lp = ((5 + L)^lp) / ((5 + 1)^lp)\n",
    "    lp = ((5 + length)**length_penalty) / ((5 + 1)**length_penalty)\n",
    "    return logprob / lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c976ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_candidates_with_scores(coca, pil_img, K=10, beam_size=5, top_p=None, temperature=1.0):\n",
    "    # 1) preprocess for CoCa\n",
    "    img_tensor = coca.preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    # 2) run your generator (implement this in your CoCa wrapper)\n",
    "    cands = coca.generate(\n",
    "        images=img_tensor,\n",
    "        num_candidates=K,\n",
    "        beam_size=beam_size,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        return_logprobs=True,   # ensure you can get per-seq logprob\n",
    "        include_eos=True\n",
    "    )\n",
    "    # cands: list of dicts: { \"tokens\": LongTensor[L], \"logprob\": float }\n",
    "    return cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e548da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def zscore(arr):\n",
    "    arr = np.asarray(arr)\n",
    "    mu, sd = arr.mean(), arr.std() + 1e-6\n",
    "    return (arr - mu) / sd\n",
    "\n",
    "def rerank(cands, img_feat, alpha=1.0, len_pen=0.7):\n",
    "    captions = []\n",
    "    lls = []\n",
    "    for d in cands:\n",
    "        cap = coca.tokenizer.decode(d[\"tokens\"])\n",
    "        captions.append(cap)\n",
    "        L = len(d[\"tokens\"])\n",
    "        lls.append(length_normalize(d[\"logprob\"], L, len_pen))\n",
    "    lls = np.array(lls, dtype=np.float32)\n",
    "\n",
    "    clips = clipscore(img_feat, captions).detach().float().cpu().numpy()\n",
    "\n",
    "    # Per-image z-normalization\n",
    "    lls_z   = zscore(lls)\n",
    "    clips_z = zscore(clips)\n",
    "\n",
    "    hybrid = lls_z + alpha * clips_z\n",
    "    idx = int(np.argmax(hybrid))\n",
    "    return captions[idx], {\n",
    "        \"chosen_idx\": idx,\n",
    "        \"captions\": captions,\n",
    "        \"lls\": lls.tolist(),\n",
    "        \"clips\": clips.tolist(),\n",
    "        \"hybrid\": hybrid.tolist()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bdcabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def run_inference(dataset, coca, alpha=0.8, K=10, beam_size=5, out_jsonl=\"preds.jsonl\"):\n",
    "    out = []\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        for img_id, img_path in dataset.iter_images():\n",
    "            pil = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            # cache CLIP image embedding\n",
    "            img_feat = clip_image_embed(pil)\n",
    "\n",
    "            cands = generate_candidates_with_scores(coca, pil, K=K, beam_size=beam_size)\n",
    "            best_caption, dbg = rerank(cands, img_feat, alpha=alpha, len_pen=0.7)\n",
    "\n",
    "            out_rec = {\n",
    "                \"image_id\": img_id,\n",
    "                \"caption\": best_caption,\n",
    "                \"dbg\": dbg\n",
    "            }\n",
    "            f.write(json.dumps(out_rec, ensure_ascii=False) + \"\\n\")\n",
    "            out.append(out_rec)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbc430bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_coco_dump(preds, out_path):\n",
    "    # preds is list of {\"image_id\": ..., \"caption\": ...}\n",
    "    dump = [{\"image_id\": p[\"image_id\"], \"caption\": p[\"caption\"]} for p in preds]\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(dump, f)\n",
    "\n",
    "# Then call the nocaps official evaluator on your json vs references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c97db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
