{
  "name": ".",
  "type": "folder",
  "children": [
    {
      "name": "README.md",
      "type": "file",
      "content": "<img src=\"./coca.png\" width=\"450px\"></img>\n\n## CoCa - Pytorch\n\nImplementation of <a href=\"https://arxiv.org/abs/2205.01917\">CoCa, Contrastive Captioners are Image-Text Foundation Models</a>, in Pytorch. They were able to elegantly fit in contrastive learning to a conventional encoder / decoder  (image to text) transformer, achieving SOTA 91.0% top-1 accuracy on ImageNet with a finetuned encoder.\n\nThis repository also chooses to adopt the specific transformer architecture from <a href=\"https://arxiv.org/abs/2204.02311\">PaLM</a>, for both the unimodal and multimodal transformers as well as the cross attention blocks (parallel SwiGLU feedforwards)\n\nUpdate: CoCa has been trained by the good folks over at <a href=\"https://github.com/mlfoundations/open_clip#fine-tuning-coca\">OpenClip</a>\n\n## Install\n\n```bash\n$ pip install coca-pytorch\n```\n\n## Usage\n\nFirst install the `vit-pytorch` for the image encoder, which needs to be pretrained\n\n```bash\n$ pip install vit-pytorch>=0.40.2\n```\n\nThen\n\n```python\nimport torch\n\n# import vision transformer\n\nfrom vit_pytorch.simple_vit_with_patch_dropout import SimpleViT\nfrom vit_pytorch.extractor import Extractor\n\nvit = SimpleViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    patch_dropout = 0.5  # https://arxiv.org/abs/2212.00794\n)\n\nvit = Extractor(vit, return_embeddings_only = True, detach = False)\n\n# extractor will enable it so the vision transformer returns its embeddings\n\n# import CoCa and instantiate it\n\nfrom coca_pytorch.coca_pytorch import CoCa\n\ncoca = CoCa(\n    dim = 512,                     # model dimension\n    img_encoder = vit,             # vision transformer - image encoder, returning image embeddings as (batch, seq, dim)\n    image_dim = 1024,              # image embedding dimension, if not the same as model dimensions\n    num_tokens = 20000,            # number of text tokens\n    unimodal_depth = 6,            # depth of the unimodal transformer\n    multimodal_depth = 6,          # depth of the multimodal transformer\n    dim_head = 64,                 # dimension per attention head\n    heads = 8,                     # number of attention heads\n    caption_loss_weight = 1.,      # weight on the autoregressive caption loss\n    contrastive_loss_weight = 1.,  # weight on the contrastive loss between image and text CLS embeddings\n).cuda()\n\n# mock text and images\n\ntext = torch.randint(0, 20000, (4, 512)).cuda()\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# train by giving CoCa your text and images with `return_loss = True`\n\nloss = coca(\n    text = text,\n    images = images,\n    return_loss = True  # set this to True to get the full caption + contrastive loss\n)\n\nloss.backward()\n\n# do the above for as much text and images...\n# then you can get the caption logits as so\n\nlogits = coca(\n    text = text,\n    images = images\n) # (4, 512, 20000)\n\n# and the CLIP-like text and image embeddings as\n\ntext_embeds, image_embeds = coca(\n    text = text,\n    images = images,\n    return_embeddings = True\n) # (4, 512), (4, 512)\n```\n\n## Citations\n\n```bibtex\n@inproceedings{Yu2022CoCaCC,\n  title   = {CoCa: Contrastive Captioners are Image-Text Foundation Models},\n  author  = {Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},\n  year    = {2022}\n}\n```\n\n```bibtex\n@inproceedings{Chowdhery2022PaLMSL,\n    title   = {PaLM: Scaling Language Modeling with Pathways},\n    author  = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Benton C. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Oliveira Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},\n    year    = {2022}\n}\n```\n"
    },
    {
      "name": "coca_pytorch",
      "type": "folder",
      "children": [
        {
          "name": "__init__.py",
          "type": "file",
          "content": "from coca_pytorch.coca_pytorch import CoCa\n"
        },
        {
          "name": "coca_pytorch.py",
          "type": "file",
          "content": "import torch\nfrom torch import einsum, nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nimport torch.distributed as dist\n\nfrom einops import rearrange, repeat\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\n# distributed\n\ndef pad_dim_to(t, length, dim = 0):\n    pad_length = length - t.shape[dim]\n    zero_pairs = (-dim - 1) if dim < 0 else (t.ndim - dim - 1)\n    return F.pad(t, (*((0, 0) * zero_pairs), 0, pad_length))\n\ndef all_gather_variable_batch(t):\n    device, rank, world_size = t.device, dist.get_rank(), dist.get_world_size()\n\n    size = torch.tensor(t.shape[0], device = device, dtype = torch.long)\n    sizes = [torch.empty_like(size, device = device, dtype = torch.long) for i in range(world_size)]\n    dist.all_gather(sizes, size)\n\n    sizes = torch.stack(sizes)\n    max_size = sizes.amax().item()\n\n    padded_t = pad_dim_to(t, max_size, dim = 0)\n    gathered_tensors = [torch.empty_like(padded_t, device = device, dtype = padded_t.dtype) for i in range(world_size)]\n    dist.all_gather(gathered_tensors, padded_t)\n\n    gathered_tensor = torch.cat(gathered_tensors)\n    seq = torch.arange(max_size, device = device)\n\n    mask = rearrange(seq, 'j -> 1 j') < rearrange(sizes, 'i -> i 1')\n    mask = rearrange(mask, 'i j -> (i j)')\n\n    gathered_tensor = gathered_tensor[mask]\n    sizes = sizes.tolist()\n\n    return gathered_tensor, sizes\n\nclass AllGather(Function):\n    @staticmethod\n    def forward(ctx, x):\n        assert dist.is_initialized() and dist.get_world_size() > 1\n        x, batch_sizes = all_gather_variable_batch(x)\n        ctx.batch_sizes = batch_sizes\n        return x\n\n    @staticmethod\n    def backward(ctx, grads):\n        batch_sizes, rank = ctx.batch_sizes, dist.get_rank()\n        grads_by_rank = grads.split(batch_sizes, dim = 0)\n        return grads_by_rank[rank]\n\nall_gather = AllGather.apply\n\n\n# normalization\n# they use layernorm without bias, something that pytorch does not offer\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\"beta\", torch.zeros(dim))\n\n    def forward(self, x):\n        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\n\n# residual\n\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, *args, **kwargs):\n        return self.fn(x, *args, **kwargs) + x\n\n# to latents\n\n\nclass EmbedToLatents(nn.Module):\n    def __init__(self, dim, dim_latents):\n        super().__init__()\n        self.to_latents = nn.Linear(dim, dim_latents, bias=False)\n\n    def forward(self, x):\n        latents = self.to_latents(x)\n        return F.normalize(latents, dim=-1)\n\n# rotary positional embedding\n# https://arxiv.org/abs/2104.09864\n\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    def forward(self, max_seq_len, *, device):\n        seq = torch.arange(max_seq_len, device=device, dtype=self.inv_freq.dtype)\n        freqs = einsum(\"i , j -> i j\", seq, self.inv_freq)\n        return torch.cat((freqs, freqs), dim=-1)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (j d) -> ... j d\", j=2)\n    x1, x2 = x.unbind(dim=-2)\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(pos, t):\n    return (t * pos.cos()) + (rotate_half(t) * pos.sin())\n\n\n# classic Noam Shazeer paper, except here they use SwiGLU instead of the more popular GEGLU for gating the feedforward\n# https://arxiv.org/abs/2002.05202\n\n\nclass SwiGLU(nn.Module):\n    def forward(self, x):\n        x, gate = x.chunk(2, dim=-1)\n        return F.silu(gate) * x\n\n\n# parallel attention and feedforward with residual\n# discovered by Wang et al + EleutherAI from GPT-J fame\n\n\nclass ParallelTransformerBlock(nn.Module):\n    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n        super().__init__()\n        self.norm = LayerNorm(dim)\n\n        attn_inner_dim = dim_head * heads\n        ff_inner_dim = dim * ff_mult\n        self.fused_dims = (attn_inner_dim, dim_head, dim_head, (ff_inner_dim * 2))\n\n        self.heads = heads\n        self.scale = dim_head**-0.5\n        self.rotary_emb = RotaryEmbedding(dim_head)\n\n        self.fused_attn_ff_proj = nn.Linear(dim, sum(self.fused_dims), bias=False)\n        self.attn_out = nn.Linear(attn_inner_dim, dim, bias=False)\n\n        self.ff_out = nn.Sequential(\n            SwiGLU(),\n            nn.Linear(ff_inner_dim, dim, bias=False)\n        )\n\n        # for caching causal mask and rotary embeddings\n\n        self.mask = None\n        self.pos_emb = None\n\n    def get_mask(self, n, device):\n        if self.mask is not None and self.mask.shape[-1] >= n:\n            return self.mask[:n, :n].to(device)\n\n        mask = torch.ones((n, n), device=device, dtype=torch.bool).triu(1)\n        self.mask = mask\n        return mask\n\n    def get_rotary_embedding(self, n, device):\n        if self.pos_emb is not None and self.pos_emb.shape[-2] >= n:\n            return self.pos_emb[:n].to(device)\n\n        pos_emb = self.rotary_emb(n, device=device)\n        self.pos_emb = pos_emb\n        return pos_emb\n\n    def forward(self, x, attn_mask=None):\n        \"\"\"\n        einstein notation\n        b - batch\n        h - heads\n        n, i, j - sequence length (base sequence length, source, target)\n        d - feature dimension\n        \"\"\"\n\n        n, device, h = x.shape[1], x.device, self.heads\n\n        # pre layernorm\n\n        x = self.norm(x)\n\n        # attention queries, keys, values, and feedforward inner\n\n        q, k, v, ff = self.fused_attn_ff_proj(x).split(self.fused_dims, dim=-1)\n\n        # split heads\n        # they use multi-query single-key-value attention, yet another Noam Shazeer paper\n        # they found no performance loss past a certain scale, and more efficient decoding obviously\n        # https://arxiv.org/abs/1911.02150\n\n        q = rearrange(q, \"b n (h d) -> b h n d\", h=h)\n\n        # rotary embeddings\n\n        positions = self.get_rotary_embedding(n, device)\n        q, k = map(lambda t: apply_rotary_pos_emb(positions, t), (q, k))\n\n        # scale\n\n        q = q * self.scale\n\n        # similarity\n\n        sim = einsum(\"b h i d, b j d -> b h i j\", q, k)\n\n        # causal mask\n\n        causal_mask = self.get_mask(n, device)\n        sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n\n        # extra attention mask - for masking out attention from text CLS token to padding\n\n        if exists(attn_mask):\n            attn_mask = rearrange(attn_mask, 'b i j -> b 1 i j')\n            sim = sim.masked_fill(~attn_mask, -torch.finfo(sim.dtype).max)\n\n        # attention\n\n        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n        attn = sim.softmax(dim=-1)\n\n        # aggregate values\n\n        out = einsum(\"b h i j, b j d -> b h i d\", attn, v)\n\n        # merge heads\n\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n        return self.attn_out(out) + self.ff_out(ff)\n\n# cross attention - using multi-query + one-headed key / values as in PaLM w/ optional parallel feedforward\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim=None,\n        dim_head=64,\n        heads=8,\n        parallel_ff=False,\n        ff_mult=4,\n        norm_context=False\n    ):\n        super().__init__()\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        inner_dim = heads * dim_head\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.context_norm = LayerNorm(context_dim) if norm_context else nn.Identity()\n\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(context_dim, dim_head * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n\n        # whether to have parallel feedforward\n\n        ff_inner_dim = ff_mult * dim\n\n        self.ff = nn.Sequential(\n            nn.Linear(dim, ff_inner_dim * 2, bias=False),\n            SwiGLU(),\n            nn.Linear(ff_inner_dim, dim, bias=False)\n        ) if parallel_ff else None\n\n    def forward(self, x, context):\n        \"\"\"\n        einstein notation\n        b - batch\n        h - heads\n        n, i, j - sequence length (base sequence length, source, target)\n        d - feature dimension\n        \"\"\"\n\n        # pre-layernorm, for queries and context\n\n        x = self.norm(x)\n        context = self.context_norm(context)\n\n        # get queries\n\n        q = self.to_q(x)\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # scale\n\n        q = q * self.scale\n\n        # get key / values\n\n        k, v = self.to_kv(context).chunk(2, dim=-1)\n\n        # query / key similarity\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k)\n\n        # attention\n\n        sim = sim - sim.amax(dim=-1, keepdim=True)\n        attn = sim.softmax(dim=-1)\n\n        # aggregate\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        # merge and combine heads\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out = self.to_out(out)\n\n        # add parallel feedforward (for multimodal layers)\n\n        if exists(self.ff):\n            out = out + self.ff(x)\n\n        return out\n\n# transformer\n\n\nclass CoCa(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        num_tokens,\n        unimodal_depth,\n        multimodal_depth,\n        dim_latents = None,\n        image_dim = None,\n        num_img_queries=256,\n        dim_head=64,\n        heads=8,\n        ff_mult=4,\n        img_encoder=None,\n        caption_loss_weight=1.,\n        contrastive_loss_weight=1.,\n        pad_id=0\n    ):\n        super().__init__()\n        self.dim = dim\n\n        self.pad_id = pad_id\n        self.caption_loss_weight = caption_loss_weight\n        self.contrastive_loss_weight = contrastive_loss_weight\n\n        # token embeddings\n\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        self.text_cls_token = nn.Parameter(torch.randn(dim))\n\n        # image encoder\n\n        self.img_encoder = img_encoder\n\n        # attention pooling for image tokens\n\n        self.img_queries = nn.Parameter(torch.randn(num_img_queries + 1, dim)) # num image queries for multimodal, but 1 extra CLS for contrastive learning\n        self.img_attn_pool = CrossAttention(dim=dim, context_dim=image_dim, dim_head=dim_head, heads=heads, norm_context=True)\n\n        self.img_attn_pool_norm = LayerNorm(dim)\n        self.text_cls_norm = LayerNorm(dim)\n\n        # to latents\n\n        dim_latents = default(dim_latents, dim)\n        self.img_to_latents = EmbedToLatents(dim, dim_latents)\n        self.text_to_latents = EmbedToLatents(dim, dim_latents)\n\n        # contrastive learning temperature\n\n        self.temperature = nn.Parameter(torch.Tensor([1.]))\n\n        # unimodal layers\n\n        self.unimodal_layers = nn.ModuleList([])\n        for ind in range(unimodal_depth):\n            self.unimodal_layers.append(\n                Residual(ParallelTransformerBlock(dim=dim, dim_head=dim_head, heads=heads, ff_mult=ff_mult)),\n            )\n\n        # multimodal layers\n\n        self.multimodal_layers = nn.ModuleList([])\n        for ind in range(multimodal_depth):\n            self.multimodal_layers.append(nn.ModuleList([\n                Residual(ParallelTransformerBlock(dim=dim, dim_head=dim_head, heads=heads, ff_mult=ff_mult)),\n                Residual(CrossAttention(dim=dim, dim_head=dim_head, heads=heads, parallel_ff=True, ff_mult=ff_mult))\n            ]))\n\n        # to logits\n\n        self.to_logits = nn.Sequential(\n            LayerNorm(dim),\n            nn.Linear(dim, num_tokens, bias=False)\n        )\n\n        # they used embedding weight tied projection out to logits, not common, but works\n        self.to_logits[-1].weight = self.token_emb.weight\n        nn.init.normal_(self.token_emb.weight, std=0.02)\n\n        # whether in data parallel setting\n        self.is_distributed = dist.is_initialized() and dist.get_world_size() > 1\n\n    def embed_text(self, text):\n        batch, device = text.shape[0], text.device\n\n        seq = text.shape[1]\n\n        text_tokens = self.token_emb(text)\n\n        # append text cls tokens\n\n        text_cls_tokens = repeat(self.text_cls_token, 'd -> b 1 d', b=batch)\n        text_tokens = torch.cat((text_tokens, text_cls_tokens), dim=-2)\n\n        # create specific mask for text cls token at the end\n        # to prevent it from attending to padding\n\n        cls_mask = rearrange(text!=self.pad_id, 'b j -> b 1 j')\n        attn_mask = F.pad(cls_mask, (0, 1, seq, 0), value=True)\n\n        # go through unimodal layers\n\n        for attn_ff in self.unimodal_layers:\n            text_tokens = attn_ff(text_tokens, attn_mask=attn_mask)\n\n        # get text cls token\n\n        text_tokens, text_cls_tokens = text_tokens[:, :-1], text_tokens[:, -1]\n        text_embeds = self.text_cls_norm(text_cls_tokens)\n        return text_embeds, text_tokens\n\n    def embed_image(self, images=None, image_tokens=None):\n        # encode images into embeddings\n        # with the img_encoder passed in at init\n        # it can also accept precomputed image tokens\n\n        assert not (exists(images) and exists(image_tokens))\n\n        if exists(images):\n            assert exists(self.img_encoder), 'img_encoder must be passed in for automatic image encoding'\n            image_tokens = self.img_encoder(images)\n\n        # attention pool image tokens\n\n        img_queries = repeat(self.img_queries, 'n d -> b n d', b=image_tokens.shape[0])\n        img_queries = self.img_attn_pool(img_queries, image_tokens)\n        img_queries = self.img_attn_pool_norm(img_queries)\n\n        return img_queries[:, 0], img_queries[:, 1:]\n\n    def forward(\n        self,\n        text,\n        images=None,\n        image_tokens=None,\n        labels=None,\n        return_loss=False,\n        return_embeddings=False\n    ):\n        batch, device = text.shape[0], text.device\n\n        if return_loss and not exists(labels):\n            text, labels = text[:, :-1], text[:, 1:]\n\n        text_embeds, text_tokens = self.embed_text(text)\n\n        image_embeds, image_tokens = self.embed_image(images=images, image_tokens=image_tokens)\n\n        # return embeddings if that is what the researcher wants\n\n        if return_embeddings:\n            return text_embeds, image_embeds\n\n        # go through multimodal layers\n\n        for attn_ff, cross_attn in self.multimodal_layers:\n            text_tokens = attn_ff(text_tokens)\n            text_tokens = cross_attn(text_tokens, image_tokens)\n\n        logits = self.to_logits(text_tokens)\n\n        if not return_loss:\n            return logits\n\n        # shorthand\n\n        ce = F.cross_entropy\n\n        # calculate caption loss (cross entropy loss)\n\n        logits = rearrange(logits, 'b n c -> b c n')\n        caption_loss = ce(logits, labels, ignore_index=self.pad_id)\n        caption_loss = caption_loss * self.caption_loss_weight\n\n        # embedding to latents\n\n        text_latents = self.text_to_latents(text_embeds)\n        image_latents = self.img_to_latents(image_embeds)\n\n        # maybe distributed all gather\n\n        if self.is_distributed:\n            latents = torch.stack((text_latents, image_latents), dim = 1)\n            latents = all_gather(latents)\n            text_latents, image_latents = latents.unbind(dim = 1)\n\n        # calculate contrastive loss\n\n        sim = einsum('i d, j d -> i j', text_latents, image_latents)\n        sim = sim * self.temperature.exp()\n        contrastive_labels = torch.arange(batch, device=device)\n\n        contrastive_loss = (ce(sim, contrastive_labels) + ce(sim.t(), contrastive_labels)) * 0.5\n        contrastive_loss = contrastive_loss * self.contrastive_loss_weight\n\n        return caption_loss + contrastive_loss\n"
        }
      ]
    }
  ]
}