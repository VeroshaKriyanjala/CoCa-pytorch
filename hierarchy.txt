{
  "name": ".",
  "type": "folder",
  "children": [
    {
      "name": "coca_pytorch",
      "type": "folder",
      "children": [
        {
          "name": "__init__.py",
          "type": "file",
          "content": "from coca_pytorch.coca_pytorch import CoCa\n"
        },
        {
          "name": "coca_pytorch.py",
          "type": "file",
          "content": "import torch\nfrom torch import einsum, nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nimport torch.distributed as dist\n\nfrom einops import rearrange, repeat\n\n# helper functions\n\ndef exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if exists(val) else d\n\n# distributed\n\ndef pad_dim_to(t, length, dim = 0):\n    pad_length = length - t.shape[dim]\n    zero_pairs = (-dim - 1) if dim < 0 else (t.ndim - dim - 1)\n    return F.pad(t, (*((0, 0) * zero_pairs), 0, pad_length))\n\ndef all_gather_variable_batch(t):\n    device, rank, world_size = t.device, dist.get_rank(), dist.get_world_size()\n\n    size = torch.tensor(t.shape[0], device = device, dtype = torch.long)\n    sizes = [torch.empty_like(size, device = device, dtype = torch.long) for i in range(world_size)]\n    dist.all_gather(sizes, size)\n\n    sizes = torch.stack(sizes)\n    max_size = sizes.amax().item()\n\n    padded_t = pad_dim_to(t, max_size, dim = 0)\n    gathered_tensors = [torch.empty_like(padded_t, device = device, dtype = padded_t.dtype) for i in range(world_size)]\n    dist.all_gather(gathered_tensors, padded_t)\n\n    gathered_tensor = torch.cat(gathered_tensors)\n    seq = torch.arange(max_size, device = device)\n\n    mask = rearrange(seq, 'j -> 1 j') < rearrange(sizes, 'i -> i 1')\n    mask = rearrange(mask, 'i j -> (i j)')\n\n    gathered_tensor = gathered_tensor[mask]\n    sizes = sizes.tolist()\n\n    return gathered_tensor, sizes\n\nclass AllGather(Function):\n    @staticmethod\n    def forward(ctx, x):\n        assert dist.is_initialized() and dist.get_world_size() > 1\n        x, batch_sizes = all_gather_variable_batch(x)\n        ctx.batch_sizes = batch_sizes\n        return x\n\n    @staticmethod\n    def backward(ctx, grads):\n        batch_sizes, rank = ctx.batch_sizes, dist.get_rank()\n        grads_by_rank = grads.split(batch_sizes, dim = 0)\n        return grads_by_rank[rank]\n\nall_gather = AllGather.apply\n\n\n# normalization\n# they use layernorm without bias, something that pytorch does not offer\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\"beta\", torch.zeros(dim))\n\n    def forward(self, x):\n        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\n\n# residual\n\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, *args, **kwargs):\n        return self.fn(x, *args, **kwargs) + x\n\n# to latents\n\n\nclass EmbedToLatents(nn.Module):\n    def __init__(self, dim, dim_latents):\n        super().__init__()\n        self.to_latents = nn.Linear(dim, dim_latents, bias=False)\n\n    def forward(self, x):\n        latents = self.to_latents(x)\n        return F.normalize(latents, dim=-1)\n\n# rotary positional embedding\n# https://arxiv.org/abs/2104.09864\n\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n    def forward(self, max_seq_len, *, device):\n        seq = torch.arange(max_seq_len, device=device, dtype=self.inv_freq.dtype)\n        freqs = einsum(\"i , j -> i j\", seq, self.inv_freq)\n        return torch.cat((freqs, freqs), dim=-1)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (j d) -> ... j d\", j=2)\n    x1, x2 = x.unbind(dim=-2)\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(pos, t):\n    return (t * pos.cos()) + (rotate_half(t) * pos.sin())\n\n\n# classic Noam Shazeer paper, except here they use SwiGLU instead of the more popular GEGLU for gating the feedforward\n# https://arxiv.org/abs/2002.05202\n\n\nclass SwiGLU(nn.Module):\n    def forward(self, x):\n        x, gate = x.chunk(2, dim=-1)\n        return F.silu(gate) * x\n\n\n# parallel attention and feedforward with residual\n# discovered by Wang et al + EleutherAI from GPT-J fame\n\n\nclass ParallelTransformerBlock(nn.Module):\n    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n        super().__init__()\n        self.norm = LayerNorm(dim)\n\n        attn_inner_dim = dim_head * heads\n        ff_inner_dim = dim * ff_mult\n        self.fused_dims = (attn_inner_dim, dim_head, dim_head, (ff_inner_dim * 2))\n\n        self.heads = heads\n        self.scale = dim_head**-0.5\n        self.rotary_emb = RotaryEmbedding(dim_head)\n\n        self.fused_attn_ff_proj = nn.Linear(dim, sum(self.fused_dims), bias=False)\n        self.attn_out = nn.Linear(attn_inner_dim, dim, bias=False)\n\n        self.ff_out = nn.Sequential(\n            SwiGLU(),\n            nn.Linear(ff_inner_dim, dim, bias=False)\n        )\n\n        # for caching causal mask and rotary embeddings\n\n        self.mask = None\n        self.pos_emb = None\n\n    def get_mask(self, n, device):\n        if self.mask is not None and self.mask.shape[-1] >= n:\n            return self.mask[:n, :n].to(device)\n\n        mask = torch.ones((n, n), device=device, dtype=torch.bool).triu(1)\n        self.mask = mask\n        return mask\n\n    def get_rotary_embedding(self, n, device):\n        if self.pos_emb is not None and self.pos_emb.shape[-2] >= n:\n            return self.pos_emb[:n].to(device)\n\n        pos_emb = self.rotary_emb(n, device=device)\n        self.pos_emb = pos_emb\n        return pos_emb\n\n    def forward(self, x, attn_mask=None):\n        \"\"\"\n        einstein notation\n        b - batch\n        h - heads\n        n, i, j - sequence length (base sequence length, source, target)\n        d - feature dimension\n        \"\"\"\n\n        n, device, h = x.shape[1], x.device, self.heads\n\n        # pre layernorm\n\n        x = self.norm(x)\n\n        # attention queries, keys, values, and feedforward inner\n\n        q, k, v, ff = self.fused_attn_ff_proj(x).split(self.fused_dims, dim=-1)\n\n        # split heads\n        # they use multi-query single-key-value attention, yet another Noam Shazeer paper\n        # they found no performance loss past a certain scale, and more efficient decoding obviously\n        # https://arxiv.org/abs/1911.02150\n\n        q = rearrange(q, \"b n (h d) -> b h n d\", h=h)\n\n        # rotary embeddings\n\n        positions = self.get_rotary_embedding(n, device)\n        q, k = map(lambda t: apply_rotary_pos_emb(positions, t), (q, k))\n\n        # scale\n\n        q = q * self.scale\n\n        # similarity\n\n        sim = einsum(\"b h i d, b j d -> b h i j\", q, k)\n\n        # causal mask\n\n        causal_mask = self.get_mask(n, device)\n        sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n\n        # extra attention mask - for masking out attention from text CLS token to padding\n\n        if exists(attn_mask):\n            attn_mask = rearrange(attn_mask, 'b i j -> b 1 i j')\n            sim = sim.masked_fill(~attn_mask, -torch.finfo(sim.dtype).max)\n\n        # attention\n\n        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n        attn = sim.softmax(dim=-1)\n\n        # aggregate values\n\n        out = einsum(\"b h i j, b j d -> b h i d\", attn, v)\n\n        # merge heads\n\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n        return self.attn_out(out) + self.ff_out(ff)\n\n# cross attention - using multi-query + one-headed key / values as in PaLM w/ optional parallel feedforward\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        *,\n        context_dim=None,\n        dim_head=64,\n        heads=8,\n        parallel_ff=False,\n        ff_mult=4,\n        norm_context=False\n    ):\n        super().__init__()\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        inner_dim = heads * dim_head\n        context_dim = default(context_dim, dim)\n\n        self.norm = LayerNorm(dim)\n        self.context_norm = LayerNorm(context_dim) if norm_context else nn.Identity()\n\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(context_dim, dim_head * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n\n        # whether to have parallel feedforward\n\n        ff_inner_dim = ff_mult * dim\n\n        self.ff = nn.Sequential(\n            nn.Linear(dim, ff_inner_dim * 2, bias=False),\n            SwiGLU(),\n            nn.Linear(ff_inner_dim, dim, bias=False)\n        ) if parallel_ff else None\n\n    def forward(self, x, context):\n        \"\"\"\n        einstein notation\n        b - batch\n        h - heads\n        n, i, j - sequence length (base sequence length, source, target)\n        d - feature dimension\n        \"\"\"\n\n        # pre-layernorm, for queries and context\n\n        x = self.norm(x)\n        context = self.context_norm(context)\n\n        # get queries\n\n        q = self.to_q(x)\n        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n\n        # scale\n\n        q = q * self.scale\n\n        # get key / values\n\n        k, v = self.to_kv(context).chunk(2, dim=-1)\n\n        # query / key similarity\n\n        sim = einsum('b h i d, b j d -> b h i j', q, k)\n\n        # attention\n\n        sim = sim - sim.amax(dim=-1, keepdim=True)\n        attn = sim.softmax(dim=-1)\n\n        # aggregate\n\n        out = einsum('b h i j, b j d -> b h i d', attn, v)\n\n        # merge and combine heads\n\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out = self.to_out(out)\n\n        # add parallel feedforward (for multimodal layers)\n\n        if exists(self.ff):\n            out = out + self.ff(x)\n\n        return out\n\n# transformer\n\n\nclass CoCa(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        num_tokens,\n        unimodal_depth,\n        multimodal_depth,\n        dim_latents = None,\n        image_dim = None,\n        num_img_queries=256,\n        dim_head=64,\n        heads=8,\n        ff_mult=4,\n        img_encoder=None,\n        caption_loss_weight=1.,\n        contrastive_loss_weight=1.,\n        pad_id=0\n    ):\n        super().__init__()\n        self.dim = dim\n\n        self.pad_id = pad_id\n        self.caption_loss_weight = caption_loss_weight\n        self.contrastive_loss_weight = contrastive_loss_weight\n\n        # token embeddings\n\n        self.token_emb = nn.Embedding(num_tokens, dim)\n        self.text_cls_token = nn.Parameter(torch.randn(dim))\n\n        # image encoder\n\n        self.img_encoder = img_encoder\n\n        # attention pooling for image tokens\n\n        self.img_queries = nn.Parameter(torch.randn(num_img_queries + 1, dim)) # num image queries for multimodal, but 1 extra CLS for contrastive learning\n        self.img_attn_pool = CrossAttention(dim=dim, context_dim=image_dim, dim_head=dim_head, heads=heads, norm_context=True)\n\n        self.img_attn_pool_norm = LayerNorm(dim)\n        self.text_cls_norm = LayerNorm(dim)\n\n        # to latents\n\n        dim_latents = default(dim_latents, dim)\n        self.img_to_latents = EmbedToLatents(dim, dim_latents)\n        self.text_to_latents = EmbedToLatents(dim, dim_latents)\n\n        # contrastive learning temperature\n\n        self.temperature = nn.Parameter(torch.Tensor([1.]))\n\n        # unimodal layers\n\n        self.unimodal_layers = nn.ModuleList([])\n        for ind in range(unimodal_depth):\n            self.unimodal_layers.append(\n                Residual(ParallelTransformerBlock(dim=dim, dim_head=dim_head, heads=heads, ff_mult=ff_mult)),\n            )\n\n        # multimodal layers\n\n        self.multimodal_layers = nn.ModuleList([])\n        for ind in range(multimodal_depth):\n            self.multimodal_layers.append(nn.ModuleList([\n                Residual(ParallelTransformerBlock(dim=dim, dim_head=dim_head, heads=heads, ff_mult=ff_mult)),\n                Residual(CrossAttention(dim=dim, dim_head=dim_head, heads=heads, parallel_ff=True, ff_mult=ff_mult))\n            ]))\n\n        # to logits\n\n        self.to_logits = nn.Sequential(\n            LayerNorm(dim),\n            nn.Linear(dim, num_tokens, bias=False)\n        )\n\n        # they used embedding weight tied projection out to logits, not common, but works\n        self.to_logits[-1].weight = self.token_emb.weight\n        nn.init.normal_(self.token_emb.weight, std=0.02)\n\n        # whether in data parallel setting\n        self.is_distributed = dist.is_initialized() and dist.get_world_size() > 1\n\n    def embed_text(self, text):\n        batch, device = text.shape[0], text.device\n\n        seq = text.shape[1]\n\n        text_tokens = self.token_emb(text)\n\n        # append text cls tokens\n\n        text_cls_tokens = repeat(self.text_cls_token, 'd -> b 1 d', b=batch)\n        text_tokens = torch.cat((text_tokens, text_cls_tokens), dim=-2)\n\n        # create specific mask for text cls token at the end\n        # to prevent it from attending to padding\n\n        cls_mask = rearrange(text!=self.pad_id, 'b j -> b 1 j')\n        attn_mask = F.pad(cls_mask, (0, 1, seq, 0), value=True)\n\n        # go through unimodal layers\n\n        for attn_ff in self.unimodal_layers:\n            text_tokens = attn_ff(text_tokens, attn_mask=attn_mask)\n\n        # get text cls token\n\n        text_tokens, text_cls_tokens = text_tokens[:, :-1], text_tokens[:, -1]\n        text_embeds = self.text_cls_norm(text_cls_tokens)\n        return text_embeds, text_tokens\n\n    def embed_image(self, images=None, image_tokens=None):\n        # encode images into embeddings\n        # with the img_encoder passed in at init\n        # it can also accept precomputed image tokens\n\n        assert not (exists(images) and exists(image_tokens))\n\n        if exists(images):\n            assert exists(self.img_encoder), 'img_encoder must be passed in for automatic image encoding'\n            image_tokens = self.img_encoder(images)\n\n        # attention pool image tokens\n\n        img_queries = repeat(self.img_queries, 'n d -> b n d', b=image_tokens.shape[0])\n        img_queries = self.img_attn_pool(img_queries, image_tokens)\n        img_queries = self.img_attn_pool_norm(img_queries)\n\n        return img_queries[:, 0], img_queries[:, 1:]\n\n    def forward(\n        self,\n        text,\n        images=None,\n        image_tokens=None,\n        labels=None,\n        return_loss=False,\n        return_embeddings=False\n    ):\n        batch, device = text.shape[0], text.device\n\n        if return_loss and not exists(labels):\n            text, labels = text[:, :-1], text[:, 1:]\n\n        text_embeds, text_tokens = self.embed_text(text)\n\n        image_embeds, image_tokens = self.embed_image(images=images, image_tokens=image_tokens)\n\n        # return embeddings if that is what the researcher wants\n\n        if return_embeddings:\n            return text_embeds, image_embeds\n\n        # go through multimodal layers\n\n        for attn_ff, cross_attn in self.multimodal_layers:\n            text_tokens = attn_ff(text_tokens)\n            text_tokens = cross_attn(text_tokens, image_tokens)\n\n        logits = self.to_logits(text_tokens)\n\n        if not return_loss:\n            return logits\n\n        # shorthand\n\n        ce = F.cross_entropy\n\n        # calculate caption loss (cross entropy loss)\n\n        logits = rearrange(logits, 'b n c -> b c n')\n        caption_loss = ce(logits, labels, ignore_index=self.pad_id)\n        caption_loss = caption_loss * self.caption_loss_weight\n\n        # embedding to latents\n\n        text_latents = self.text_to_latents(text_embeds)\n        image_latents = self.img_to_latents(image_embeds)\n\n        # maybe distributed all gather\n\n        if self.is_distributed:\n            latents = torch.stack((text_latents, image_latents), dim = 1)\n            latents = all_gather(latents)\n            text_latents, image_latents = latents.unbind(dim = 1)\n\n        # calculate contrastive loss\n\n        sim = einsum('i d, j d -> i j', text_latents, image_latents)\n        sim = sim * self.temperature.exp()\n        contrastive_labels = torch.arange(batch, device=device)\n\n        contrastive_loss = (ce(sim, contrastive_labels) + ce(sim.t(), contrastive_labels)) * 0.5\n        contrastive_loss = contrastive_loss * self.contrastive_loss_weight\n\n        return caption_loss + contrastive_loss\n"
        }
      ]
    },
    {
      "name": "data",
      "type": "folder",
      "children": [
        {
          "name": "nocaps",
          "type": "folder",
          "children": [
            {
              "name": "annotations",
              "type": "folder",
              "children": []
            },
            {
              "name": "images",
              "type": "folder",
              "children": []
            }
          ]
        }
      ]
    },
    {
      "name": "main.py",
      "type": "file",
      "content": "import torch\n\n# import vision transformer\n\nfrom vit_pytorch.simple_vit_with_patch_dropout import SimpleViT\nfrom vit_pytorch.extractor import Extractor\n\nvit = SimpleViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    patch_dropout = 0.5  # https://arxiv.org/abs/2212.00794\n)\n\nvit = Extractor(vit, return_embeddings_only = True, detach = False)\n\n# extractor will enable it so the vision transformer returns its embeddings\n\n# import CoCa and instantiate it\n\nfrom coca_pytorch.coca_pytorch import CoCa\n\ncoca = CoCa(\n    dim = 512,                     # model dimension\n    img_encoder = vit,             # vision transformer - image encoder, returning image embeddings as (batch, seq, dim)\n    image_dim = 1024,              # image embedding dimension, if not the same as model dimensions\n    num_tokens = 20000,            # number of text tokens\n    unimodal_depth = 6,            # depth of the unimodal transformer\n    multimodal_depth = 6,          # depth of the multimodal transformer\n    dim_head = 64,                 # dimension per attention head\n    heads = 8,                     # number of attention heads\n    caption_loss_weight = 1.,      # weight on the autoregressive caption loss\n    contrastive_loss_weight = 1.,  # weight on the contrastive loss between image and text CLS embeddings\n).cuda()\n\n# mock text and images\n\ntext = torch.randint(0, 20000, (4, 512)).cuda()\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# train by giving CoCa your text and images with `return_loss = True`\n\nloss = coca(\n    text = text,\n    images = images,\n    return_loss = True  # set this to True to get the full caption + contrastive loss\n)\n\nloss.backward()\n\n# do the above for as much text and images...\n# then you can get the caption logits as so\n\nlogits = coca(\n    text = text,\n    images = images\n) # (4, 512, 20000)\n\n# and the CLIP-like text and image embeddings as\n\ntext_embeds, image_embeds = coca(\n    text = text,\n    images = images,\n    return_embeddings = True\n) # (4, 512), (4, 512)\n\n\nprint(\"Loss:\", loss.item())\nprint(\"Logits shape:\", logits.shape)\nprint(\"Text embeds:\", text_embeds.shape)\nprint(\"Image embeds:\", image_embeds.shape)\n"
    },
    {
      "name": "outputs",
      "type": "folder",
      "children": []
    },
    {
      "name": "rerank",
      "type": "folder",
      "children": [
        {
          "name": "__init__.py",
          "type": "file",
          "content": ""
        },
        {
          "name": "clip_score.py",
          "type": "file",
          "content": "import torch\nimport open_clip\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n_model, _pre, _tok = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n_model = _model.to(device).eval()\n\n@torch.no_grad()\ndef clip_image_embed(pil_img):\n    t = _pre(pil_img).unsqueeze(0).to(device)\n    f = _model.encode_image(t)\n    return (f / f.norm(dim=-1, keepdim=True)).squeeze(0)\n\n@torch.no_grad()\ndef clip_text_embed(texts):\n    tokens = _tok(texts).to(device)\n    f = _model.encode_text(tokens)\n    return f / f.norm(dim=-1, keepdim=True)\n\n@torch.no_grad()\ndef clip_score(img_feat, captions):\n    txt_feat = clip_text_embed(captions)\n    return (txt_feat @ img_feat.unsqueeze(1)).squeeze(1)  # cosine\n"
        },
        {
          "name": "coca_wrapper.py",
          "type": "file",
          "content": "from typing import List, Dict, Any\nimport torch\nfrom PIL import Image\n\nclass CoCaWrapper:\n    \"\"\"\n    Fill the 3 TODOs to use your CoCa model directly.\n    \"\"\"\n    def __init__(self, device=None):\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        # TODO 1: load your CoCa + tokenizer + preprocess\n        # Example (PSEUDOCODE):\n        # from coca_pytorch.coca_pytorch import CoCa, get_tokenizer, get_preprocess\n        # self.model = CoCa.from_pretrained(\"...\").to(self.device).eval()\n        # self.tokenizer = get_tokenizer()\n        # self.preprocess = get_preprocess()\n\n    def preprocess_image(self, pil_img: Image.Image) -> torch.Tensor:\n        # TODO 2: return (1, C, H, W) tensor on self.device\n        # return self.preprocess(pil_img).unsqueeze(0).to(self.device)\n        raise NotImplementedError(\"Implement preprocess_image()\")\n\n    @torch.no_grad()\n    def generate(self, image_tensor: torch.Tensor, num_candidates=10, beam_size=5,\n                 top_p=None, temperature=1.0) -> List[Dict[str, Any]]:\n        \"\"\"\n        Return a list of dicts: { \"tokens\": LongTensor[L], \"logprob\": float }\n        logprob should be the sum of token log-probs (include EOS).\n        \"\"\"\n        # TODO 3: call your model’s generate API and compute each sequence logprob.\n        # You may already get logprobs from your generator; if not, request per-token scores.\n        raise NotImplementedError(\"Implement generate()\")\n\n    def detokenize(self, token_ids) -> str:\n        # TODO 4: ids -> string\n        # return self.tokenizer.decode(token_ids)\n        raise NotImplementedError(\"Implement detokenize()\")\n"
        },
        {
          "name": "utils.py",
          "type": "file",
          "content": "import numpy as np\n\ndef length_normalize(logprob: float, length: int, length_penalty: float = 0.7) -> float:\n    lp = ((5 + length) ** length_penalty) / ((5 + 1) ** length_penalty)\n    return logprob / lp\n\ndef zscore(x):\n    x = np.asarray(x, dtype=np.float32)\n    m, s = x.mean(), x.std()\n    return (x - m) / (s + 1e-6)\n"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "folder",
      "children": [
        {
          "name": "download_nocaps_images.py",
          "type": "file",
          "content": "import json, argparse, subprocess\nfrom pathlib import Path\n\ndef collect_ids(ann_paths):\n    ids = set()\n    for p in ann_paths:\n        js = json.load(open(p, \"r\", encoding=\"utf-8\"))\n        # nocaps JSON usually has an \"images\" list; each item has an \"id\" or \"open_images_id\"\n        for im in js.get(\"images\", []):\n            # Try common fields; adjust if your JSON uses a different key\n            oid = im.get(\"open_images_id\") or im.get(\"id\") or im.get(\"image_id\") or im.get(\"file_name\")\n            if oid:\n                # strip extension if present\n                oid = Path(str(oid)).stem\n                ids.add(oid)\n    return sorted(ids)\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--ann\", nargs=\"+\", required=True, help=\"Paths to nocaps annotation JSON(s)\")\n    ap.add_argument(\"--out_dir\", default=\"data/nocaps/images\")\n    args = ap.parse_args()\n\n    out_dir = Path(args.out_dir)\n    out_dir.mkdir(parents=True, exist_ok=True)\n    ids = collect_ids(args.ann)\n    print(f\"Total unique image IDs: {len(ids)}\")\n\n    # Use the openimages CLI to download by exact IDs (it pulls from Google’s public bucket).\n    # It auto-creates subfolders; we point -d to out_dir.\n    # Note: the CLI accepts many IDs; we chunk to avoid shell limits.\n    chunk = 200\n    for i in range(0, len(ids), chunk):\n        part = ids[i:i+chunk]\n        cmd = [\"openimages\", \"download\", \"--image_ids\"] + part + [\"-d\", str(out_dir)]\n        print(\"Running:\", \" \".join(cmd[:5]), f\"... (+{len(part)} ids)\")\n        subprocess.run(cmd, check=True)\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "eval_coco.py",
          "type": "file",
          "content": "import argparse\nfrom pycocotools.coco import COCO\nfrom pycocoevalcap.eval import COCOEvalCap\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--ann\", required=True)   # nocaps annotations (COCO-style)\n    ap.add_argument(\"--pred\", required=True)  # our predictions (COCO-style)\n    args = ap.parse_args()\n\n    coco = COCO(args.ann)\n    cocoRes = coco.loadRes(args.pred)\n    cocoEval = COCOEvalCap(coco, cocoRes)\n    cocoEval.evaluate()\n    print(\"== Overall ==\")\n    for m, v in cocoEval.eval.items():\n        print(f\"{m:10s}: {v:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "jsonl_to_coco.py",
          "type": "file",
          "content": "import json, argparse\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--pred_jsonl\", required=True)\n    ap.add_argument(\"--out_json\", required=True)\n    args = ap.parse_args()\n    out = []\n    with open(args.pred_jsonl, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            r = json.loads(line)\n            out.append({\"image_id\": r[\"image_id\"], \"caption\": r[\"caption\"]})\n    with open(args.out_json, \"w\", encoding=\"utf-8\") as f:\n        json.dump(out, f)\n    print(f\"Wrote {len(out)} predictions → {args.out_json}\")\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "rerank.py",
          "type": "file",
          "content": "import json, argparse, numpy as np\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom tabulate import tabulate\n\nfrom rerank.coca_wrapper import CoCaWrapper\nfrom rerank.clip_score import clip_image_embed, clip_score\nfrom rerank.utils import length_normalize, zscore\n\ndef rerank_one(pil_img, coca, K=10, beam=5, top_p=None, temperature=1.0, alpha=0.8, len_pen=0.7):\n    img_feat = clip_image_embed(pil_img)\n    img_tensor = coca.preprocess_image(pil_img)\n    cands = coca.generate(img_tensor, num_candidates=K, beam_size=beam, top_p=top_p, temperature=temperature)\n\n    caps, lls = [], []\n    for d in cands:\n        caps.append(coca.detokenize(d[\"tokens\"]))\n        L = int(len(d[\"tokens\"]))\n        lls.append(length_normalize(float(d[\"logprob\"]), L, len_pen))\n    clips = clip_score(img_feat, caps).detach().cpu().numpy()\n\n    lls_z, clips_z = zscore(lls), zscore(clips)\n    hybrid = lls_z + alpha * clips_z\n    i = int(hybrid.argmax())\n    return caps[i], {\"captions\": caps, \"ll\": list(lls), \"clip\": list(clips), \"hybrid\": list(hybrid), \"chosen_idx\": i}\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--images_dir\", required=True)\n    ap.add_argument(\"--list_file\", required=True)\n    ap.add_argument(\"--out_jsonl\", default=\"outputs/preds.jsonl\")\n    ap.add_argument(\"--alpha\", type=float, default=0.8)\n    ap.add_argument(\"--K\", type=int, default=10)\n    ap.add_argument(\"--beam\", type=int, default=5)\n    ap.add_argument(\"--top_p\", type=float, default=None)\n    ap.add_argument(\"--temperature\", type=float, default=1.0)\n    args = ap.parse_args()\n\n    Path(\"outputs\").mkdir(parents=True, exist_ok=True)\n    coca = CoCaWrapper()\n\n    rows = []\n    with open(args.out_jsonl, \"w\", encoding=\"utf-8\") as fout, open(args.list_file, \"r\", encoding=\"utf-8\") as fin:\n        for line in tqdm(fin, desc=\"Reranking\"):\n            fname = line.strip()\n            if not fname: continue\n            pil = Image.open(Path(args.images_dir) / fname).convert(\"RGB\")\n            cap, dbg = rerank_one(pil, coca, K=args.K, beam=args.beam, top_p=args.top_p,\n                                  temperature=args.temperature, alpha=args.alpha)\n            fout.write(json.dumps({\"image_id\": fname, \"caption\": cap, \"dbg\": dbg}, ensure_ascii=False) + \"\\n\")\n            rows.append([fname, cap[:80]])\n    print(tabulate(rows[:10], headers=[\"image_id\", \"caption\"]))\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    }
  ]
}