{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40ca89d",
   "metadata": {},
   "source": [
    "\n",
    "# NoCaps Validation â€” CoCa Custom Sampling + **Hybrid Reranking** (OpenCLIP)\n",
    "\n",
    "Pipeline:\n",
    "1. Load **NoCaps validation** annotations + images.  \n",
    "2. Load **OpenCLIP CoCa** (caption generation).  \n",
    "3. **Custom sampling** loop (temperature, top-k, top-p, no-repeat-ngram) for *N* diverse candidates.  \n",
    "4. Load **CLIP ViT-B/32** (scoring).  \n",
    "5. **Hybrid reranking**:  \n",
    "   \\[ Score(c) = \\log P_{\\text{CoCa}}(c\\mid I) + \\alpha \\cdot \\text{CLIPScore}(I,c) \\]  \n",
    "6. Evaluate with BLEU, METEOR, ROUGE_L, CIDEr (SPICE skipped).  \n",
    "7. Show qualitative examples.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install open_clip_torch pillow tqdm torchvision pycocotools\n",
    "%pip install git+https://github.com/salaniz/pycocoevalcap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4665b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import open_clip\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "# === paths ===\n",
    "ANN_PATH = \"data/nocap_val_4500_captions.json\"\n",
    "IMG_DIR  = \"data/validation\"\n",
    "\n",
    "# hyperparams\n",
    "N_CANDIDATES = 5\n",
    "SEQ_LEN = 28\n",
    "TEMP = 1.1\n",
    "TOP_K = 50\n",
    "TOP_P = 0.9\n",
    "NO_REPEAT_N = 3\n",
    "\n",
    "ALPHA = 2.0\n",
    "LEN_NORM = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90e165f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 4500\n",
      "Example refs: ['A baby is standing in front of a house.', 'A little girl in a white jacket and sandals.', 'A young child stands in front of a house.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(ANN_PATH, \"r\") as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "id2file = {img[\"id\"]: img[\"file_name\"] for img in ann[\"images\"]}\n",
    "caps_by_id = defaultdict(list)\n",
    "for a in ann[\"annotations\"]:\n",
    "    caps_by_id[a[\"image_id\"]].append(a[\"caption\"])\n",
    "\n",
    "print(\"Images:\", len(ann[\"images\"]))\n",
    "print(\"Example refs:\", caps_by_id[ann[\"images\"][0][\"id\"]][:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ef2ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "coca_model, _, coca_preprocess = open_clip.create_model_and_transforms(\"coca_ViT-L-14\", pretrained=\"mscoco_finetuned_laion2b_s13b_b90k\")\n",
    "coca_model = coca_model.to(device).eval()\n",
    "coca_tokenizer = open_clip.get_tokenizer(\"coca_ViT-L-14\")\n",
    "\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "clip_tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "from open_clip import tokenizer as openclip_tok_mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3782d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special IDs: 49406 49407 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _detect_special_ids(tok):\n",
    "    bos_id, eos_id, pad_id = 49406, 49407, 0\n",
    "    return bos_id, eos_id, pad_id\n",
    "\n",
    "BOS_ID, EOS_ID, PAD_ID = _detect_special_ids(coca_tokenizer)\n",
    "print(\"Special IDs:\", BOS_ID, EOS_ID, PAD_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc37f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_k_top_p_filtering(logits, top_k=None, top_p=None):\n",
    "    filtered = logits.clone()\n",
    "    if top_k and top_k > 0:\n",
    "        kth_vals, _ = torch.topk(filtered, top_k)\n",
    "        thresh = kth_vals[..., -1].unsqueeze(-1)\n",
    "        filtered[filtered < thresh] = -float(\"inf\")\n",
    "    if top_p and 0.0 < top_p < 1.0:\n",
    "        sorted_vals, sorted_idx = torch.sort(filtered, descending=True)\n",
    "        probs = torch.softmax(sorted_vals, dim=-1)\n",
    "        cumprobs = torch.cumsum(probs, dim=-1)\n",
    "        mask = cumprobs > top_p\n",
    "        mask[..., 0] = False\n",
    "        sorted_vals[mask] = -float(\"inf\")\n",
    "        unsort = torch.empty_like(sorted_idx)\n",
    "        unsort.scatter_(0, sorted_idx, torch.arange(sorted_idx.numel(), device=sorted_idx.device))\n",
    "        filtered = sorted_vals[unsort]\n",
    "    return filtered\n",
    "\n",
    "def violates_no_repeat(ids, next_id, n):\n",
    "    if not n or n <= 0 or len(ids) < n-1:\n",
    "        return False\n",
    "    ngram = ids[-(n-1):] + [next_id]\n",
    "    for i in range(len(ids) - (n-1)):\n",
    "        if ids[i:i+n] == ngram:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "062f2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def coca_sample_once(pil_img, max_len=30, temperature=1.0, top_k=50, top_p=0.9, no_repeat_ngram_size=3):\n",
    "    img = coca_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    seq = torch.tensor([[BOS_ID]], device=device)\n",
    "    for _ in range(max_len):\n",
    "        logits = coca_model(image=img, text=seq)\n",
    "        next_logits = logits[:, -1, :].squeeze(0)\n",
    "        next_logits = next_logits / temperature\n",
    "        next_logits = top_k_top_p_filtering(next_logits, top_k=top_k, top_p=top_p)\n",
    "        ids_list = seq.squeeze(0).tolist()\n",
    "        if no_repeat_ngram_size:\n",
    "            for tok_id in range(next_logits.numel()):\n",
    "                if violates_no_repeat(ids_list, tok_id, no_repeat_ngram_size):\n",
    "                    next_logits[tok_id] = -float(\"inf\")\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, 1)\n",
    "        seq = torch.cat([seq, next_id.view(1,1)], dim=1)\n",
    "        if int(next_id.item()) == EOS_ID:\n",
    "            break\n",
    "    ids = seq.squeeze(0).tolist()\n",
    "    if ids and ids[0] == BOS_ID: ids = ids[1:]\n",
    "    if EOS_ID in ids: ids = ids[:ids.index(EOS_ID)]\n",
    "    return openclip_tok_mod.decode(torch.tensor(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "228734ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_n_candidates(pil_img, N=5):\n",
    "    caps = [coca_sample_once(pil_img, max_len=SEQ_LEN, temperature=TEMP, top_k=TOP_K, top_p=TOP_P, no_repeat_ngram_size=NO_REPEAT_N) for _ in range(N)]\n",
    "    uniq = list(dict.fromkeys(caps))\n",
    "    return uniq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "945c6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def coca_loglik(pil_img, caption):\n",
    "    img = coca_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    toks = coca_tokenizer([caption]).to(device)\n",
    "    input_ids, target_ids = toks[:, :-1], toks[:, 1:]\n",
    "    logits = coca_model(image=img, text=input_ids)\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    token_ll = logp.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    mask = (target_ids != 0)\n",
    "    ll_sum = (token_ll * mask).sum().item()\n",
    "    tokens_kept = int(mask.sum().item())\n",
    "    return ll_sum / tokens_kept if LEN_NORM else ll_sum\n",
    "\n",
    "@torch.no_grad()\n",
    "def hybrid_rerank(pil_img, candidates, alpha=2.0):\n",
    "    img = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    img_feat = clip_model.encode_image(img); img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "    toks = clip_tokenizer(candidates).to(device)\n",
    "    txt_feat = clip_model.encode_text(toks); txt_feat /= txt_feat.norm(dim=-1, keepdim=True)\n",
    "    clip_sims = (img_feat @ txt_feat.T).squeeze(0).cpu().tolist()\n",
    "    rows = []\n",
    "    for c, cs in zip(candidates, clip_sims):\n",
    "        ll = coca_loglik(pil_img, c)\n",
    "        score = ll + alpha * cs\n",
    "        rows.append((c, ll, cs, score))\n",
    "    rows.sort(key=lambda x: x[-1], reverse=True)\n",
    "    return rows[0][0], rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a035ade",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(slice(None, None, None), -1, slice(None, None, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m test_path = Path(IMG_DIR) / id2file[ann[\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m      2\u001b[39m pil = Image.open(test_path).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m cands = \u001b[43mgenerate_n_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m best, ranked = hybrid_rerank(pil, cands, alpha=ALPHA)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCandidates:\u001b[39m\u001b[33m\"\u001b[39m, cands)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mgenerate_n_candidates\u001b[39m\u001b[34m(pil_img, N)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_n_candidates\u001b[39m(pil_img, N=\u001b[32m5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     caps = [\u001b[43mcoca_sample_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_P\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNO_REPEAT_N\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N)]\n\u001b[32m      3\u001b[39m     uniq = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mdict\u001b[39m.fromkeys(caps))\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m uniq\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcoca_sample_once\u001b[39m\u001b[34m(pil_img, max_len, temperature, top_k, top_p, no_repeat_ngram_size)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len):\n\u001b[32m      6\u001b[39m     logits = coca_model(image=img, text=seq)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     next_logits = \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m.squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m      8\u001b[39m     next_logits = next_logits / temperature\n\u001b[32m      9\u001b[39m     next_logits = top_k_top_p_filtering(next_logits, top_k=top_k, top_p=top_p)\n",
      "\u001b[31mKeyError\u001b[39m: (slice(None, None, None), -1, slice(None, None, None))"
     ]
    }
   ],
   "source": [
    "\n",
    "test_path = Path(IMG_DIR) / id2file[ann[\"images\"][0][\"id\"]]\n",
    "pil = Image.open(test_path).convert(\"RGB\")\n",
    "cands = generate_n_candidates(pil, N=5)\n",
    "best, ranked = hybrid_rerank(pil, cands, alpha=ALPHA)\n",
    "print(\"Candidates:\", cands)\n",
    "print(\"Best:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = []\n",
    "for img_info in tqdm(ann[\"images\"][:50], desc=\"Demo subset\"):\n",
    "    fpath = Path(IMG_DIR) / img_info[\"file_name\"]\n",
    "    if not fpath.exists(): continue\n",
    "    pil = Image.open(fpath).convert(\"RGB\")\n",
    "    cands = generate_n_candidates(pil, N=N_CANDIDATES)\n",
    "    best, _ = hybrid_rerank(pil, cands, alpha=ALPHA)\n",
    "    preds.append({\"image_id\": img_info[\"id\"], \"caption\": best})\n",
    "\n",
    "OUT_JSON = \"preds_demo.json\"\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(preds, f)\n",
    "\n",
    "coco = COCO(ANN_PATH)\n",
    "cocoRes = coco.loadRes(OUT_JSON)\n",
    "evaluator = COCOEvalCap(coco, cocoRes)\n",
    "evaluator.scorers = [(Bleu(4), [\"Bleu_1\",\"Bleu_2\",\"Bleu_3\",\"Bleu_4\"]), (Meteor(),\"METEOR\"), (Rouge(),\"ROUGE_L\"), (Cider(),\"CIDEr\")]\n",
    "evaluator.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
