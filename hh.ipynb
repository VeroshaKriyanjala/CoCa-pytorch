{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoCaps Validation â€” OpenCLIP CoCa End-to-End (Option A)\n",
    "\n",
    "This notebook:\n",
    "1. Installs dependencies (OpenCLIP CoCa + COCO caption eval).\n",
    "2. Loads NoCaps **validation** annotations (expects 10 captions per image).\n",
    "3. Loads a **pretrained CoCa** from OpenCLIP.\n",
    "4. Generates captions (beam search) for all images.\n",
    "5. Evaluates with **BLEU, METEOR, ROUGE_L, CIDEr** (SPICE optional).\n",
    "\n",
    "> **Paths to set:** `ANN_PATH` and `IMG_DIR` near the top.  \n",
    "> **Expected files:**  \n",
    ">  - `data/nocap_val_4500_captions.json`  \n",
    ">  - `data/validation/<image files>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.12/site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: open_clip_torch in ./venv/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.12/site-packages (11.3.0)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.12/site-packages (0.17.0+cu118)\n",
      "Requirement already satisfied: pycocotools in ./venv/lib/python3.12/site-packages (2.0.10)\n",
      "Requirement already satisfied: torch>=2.0 in ./venv/lib/python3.12/site-packages (from open_clip_torch) (2.2.0+cu118)\n",
      "Requirement already satisfied: regex in ./venv/lib/python3.12/site-packages (from open_clip_torch) (2025.9.18)\n",
      "Requirement already satisfied: ftfy in ./venv/lib/python3.12/site-packages (from open_clip_torch) (6.3.1)\n",
      "Requirement already satisfied: huggingface-hub in ./venv/lib/python3.12/site-packages (from open_clip_torch) (0.35.1)\n",
      "Requirement already satisfied: safetensors in ./venv/lib/python3.12/site-packages (from open_clip_torch) (0.6.2)\n",
      "Requirement already satisfied: timm>=1.0.17 in ./venv/lib/python3.12/site-packages (from open_clip_torch) (1.0.20)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from torchvision) (2.32.5)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in ./venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.8.86)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.12/site-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from ftfy->open_clip_torch) (0.2.14)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (1.1.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->torchvision) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->torchvision) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->torchvision) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->torchvision) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->torch>=2.0->open_clip_torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
      "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-_xvidsir\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-_xvidsir\n",
      "  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in ./venv/lib/python3.12/site-packages (from pycocoevalcap==1.2) (2.0.10)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (2.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# --- Install dependencies (internet required) ---\n",
    "%pip install --upgrade pip\n",
    "%pip install open_clip_torch pillow tqdm torchvision pycocotools\n",
    "%pip install git+https://github.com/salaniz/pycocoevalcap\n",
    "# Optional for SPICE (Java required):\n",
    "# !apt-get update && apt-get install -y default-jre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_347372/2589801883.py\", line 5, in <module>\n",
      "    import torch\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/torch/__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import open_clip\n",
    "from torchvision import transforms\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "# ---- Set your paths here ----\n",
    "ANN_PATH = \"data/nocap_val_4500_captions.json\"   # NoCaps validation annotations\n",
    "IMG_DIR  = \"data/validation\"                      # folder containing validation images\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "assert Path(ANN_PATH).exists(), f\"Annotation file not found: {ANN_PATH}\"\n",
    "assert Path(IMG_DIR).exists(), f\"Image folder not found: {IMG_DIR}\"\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# images: 4500\n",
      "min refs: 10, max refs: 10, mean refs: 10.00\n",
      "non-10 reference counts: 0\n",
      "Example image: 0013ea2087020901.jpg\n",
      "Refs: ['A baby is standing in front of a house.', 'A little girl in a white jacket and sandals.', 'A young child stands in front of a house.', 'A child is wearing a white shirt and standing on a side walk. ', 'A little boy is standing in his diaper with a white shirt on.', 'A child wearing a diaper and shoes stands on the sidewalk.', 'A child is wearing a light-colored shirt during the daytime.', 'A little kid standing on the pavement in a shirt. ', 'Black and white photo of a little girl smiling.', 'a cute baby is standing alone with white shirt']\n"
     ]
    }
   ],
   "source": [
    "# --- Load annotations + verify 10 refs per image ---\n",
    "with open(ANN_PATH, \"r\") as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "id2file = {img[\"id\"]: img[\"file_name\"] for img in ann[\"images\"]}\n",
    "caps_by_id = defaultdict(list)\n",
    "for a in ann[\"annotations\"]:\n",
    "    caps_by_id[a[\"image_id\"]].append(a[\"caption\"])\n",
    "\n",
    "num_images = len(ann[\"images\"])\n",
    "lens = [len(caps_by_id[i[\"id\"]]) for i in ann[\"images\"]]\n",
    "\n",
    "print(f\"# images: {num_images}\")\n",
    "print(f\"min refs: {min(lens)}, max refs: {max(lens)}, mean refs: {sum(lens)/len(lens):.2f}\")\n",
    "\n",
    "bad = [(i[\"id\"], id2file[i[\"id\"]], len(caps_by_id[i[\"id\"]])) for i in ann[\"images\"] if len(caps_by_id[i[\"id\"]]) != 10]\n",
    "print(\"non-10 reference counts:\", len(bad))\n",
    "if not bad:\n",
    "    first = ann[\"images\"][0]\n",
    "    print(\"Example image:\", first[\"file_name\"])\n",
    "    print(\"Refs:\", caps_by_id[first[\"id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae616ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import open_clip\n",
    "# open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Load OpenCLIP CoCa ---\n",
    "\n",
    "# ''' \n",
    "# ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
    "# ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
    "# ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
    "# ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
    "# '''\n",
    "\n",
    "# model_name = \"coca_ViT-L-14\"\n",
    "# pretrained_tag = \"mscoco_finetuned_laion2b_s13b_b90k\"\n",
    "\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained_tag)\n",
    "# tokenizer = open_clip.get_tokenizer(model_name)\n",
    "# model = model.to(device).eval()\n",
    "\n",
    "# print(\"Loaded:\", model_name, \"/\", pretrained_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c530edce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse_g3/CoCa-pytorch/venv/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# A separate CLIP model for scoring (small & fast: ViT-B/32)\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"openai\"  # or \"laion2b_s34b_b79k\" if you prefer open LAION weights\n",
    ")\n",
    "clip_tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def clipscore(pil_img, caption: str) -> float:\n",
    "    # Encode image once\n",
    "    img = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_feat = clip_model.encode_image(img)\n",
    "        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Encode caption\n",
    "    txt = clip_tokenizer([caption]).to(device)\n",
    "    with torch.no_grad():\n",
    "        txt_feat = clip_model.encode_text(txt)\n",
    "        txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # cosine similarity\n",
    "    sim = (img_feat @ txt_feat.T).item()\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a15fad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    from open_clip import tokenizer as openclip_tok_mod\n",
    "    _have_openclip_decoder = hasattr(openclip_tok_mod, \"decode\")\n",
    "except Exception:\n",
    "    openclip_tok_mod = None\n",
    "    _have_openclip_decoder = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption_openclip(pil_img, max_len=30, temperature=1.0, model_name=None, pretrained_tag=None, top_k=None, top_p=None):\n",
    "    \n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained_tag)\n",
    "    tokenizer = open_clip.get_tokenizer(model_name)\n",
    "    model = model.to(device).eval()\n",
    "    \n",
    "    img = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    tried = []\n",
    "    out = None\n",
    "    for kwargs in (\n",
    "        dict(seq_len=max_len, temperature=temperature, top_k=top_k, top_p=top_p),\n",
    "        dict(seq_len=max_len, temperature=temperature),\n",
    "        dict(max_len=max_len, temperature=temperature),\n",
    "        dict(seq_len=max_len),\n",
    "        dict(max_len=max_len),\n",
    "        dict(),\n",
    "    ):\n",
    "        try:\n",
    "            out = model.generate(img, **{k: v for k, v in kwargs.items() if v is not None})\n",
    "            break\n",
    "        except TypeError as e:\n",
    "            tried.append(str(e))\n",
    "            out = None\n",
    "\n",
    "    if out is None:\n",
    "        raise RuntimeError(\"open_clip CoCa.generate() signature not recognized. Tried:\\n\" + \"\\n\".join(tried))\n",
    "\n",
    "    # --- decode handling ---\n",
    "    if isinstance(out, list):\n",
    "        if len(out) and isinstance(out[0], str):\n",
    "            return out[0]\n",
    "        if len(out) and torch.is_tensor(out[0]):\n",
    "            ids = out[0]\n",
    "        elif len(out) and isinstance(out[0], (list, tuple)):\n",
    "            ids = torch.tensor(out[0])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unexpected list return type from model.generate(): {type(out[0])}\")\n",
    "    elif torch.is_tensor(out):\n",
    "        ids = out[0]\n",
    "    else:\n",
    "        return str(out)\n",
    "\n",
    "    if not torch.is_tensor(ids):\n",
    "        ids = torch.tensor(ids)\n",
    "    if _have_openclip_decoder:\n",
    "        return openclip_tok_mod.decode(ids)\n",
    "    if hasattr(model, \"tokenizer\") and hasattr(model.tokenizer, \"decode\"):\n",
    "        return model.tokenizer.decode(ids.tolist())\n",
    "\n",
    "    raise RuntimeError(\"model.generate returned token IDs but no decoder is available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a1ffdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_n_candidates(pil_img, seq_len=28, temperature=0.9, N=4):\n",
    "    \"\"\"\n",
    "    Calls your existing generate_caption_openclip N times to get diverse candidates.\n",
    "    NOTE: If your build is purely greedy, multiple calls may be identical.\n",
    "    Diversity relies on temperature / stochastic decoding in your OpenCLIP build.\n",
    "    \"\"\"\n",
    "    cands = []\n",
    "    for _ in range(N):\n",
    "        if _ == 0:\n",
    "            model_name = \"coca_ViT-B-32\"\n",
    "            pretrained_tag = \"laion2b_s13b_b90k\"\n",
    "            print(\"1\")\n",
    "        elif _ == 1:\n",
    "            model_name = \"coca_ViT-B-32\"\n",
    "            pretrained_tag = \"mscoco_finetuned_laion2b_s13b_b90k\"\n",
    "            print(\"2\")\n",
    "        elif _ == 2:\n",
    "            model_name = \"coca_ViT-L-14\"\n",
    "            pretrained_tag = \"laion2b_s13b_b90k\"\n",
    "            print(\"3\")\n",
    "        else:\n",
    "            model_name = \"coca_ViT-L-14\"\n",
    "            pretrained_tag = \"mscoco_finetuned_laion2b_s13b_b90k\"\n",
    "            print(\"4\")\n",
    "            \n",
    "        cap = generate_caption_openclip(\n",
    "            pil_img,\n",
    "            max_len=seq_len,         # or seq_len=seq_len in your wrapper, both handled\n",
    "            temperature=temperature,  # >1.0 = more diverse; <1.0 = safer\n",
    "            model_name=model_name,\n",
    "            pretrained_tag=pretrained_tag,\n",
    "        )\n",
    "        cands.append(cap)\n",
    "        print(f\"Candidate {_}: {cap}\")\n",
    "    # Deduplicate while keeping order\n",
    "    seen = set(); uniq = []\n",
    "    for c in cands:\n",
    "        if c not in seen:\n",
    "            seen.add(c); uniq.append(c)\n",
    "    return uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93839585",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clip_rerank(pil_img, candidates):\n",
    "    # Cache image feature once\n",
    "    img = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    img_feat = clip_model.encode_image(img)\n",
    "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Encode all captions together (batched)\n",
    "    if not candidates:\n",
    "        return None, []\n",
    "    toks = clip_tokenizer(candidates).to(device)\n",
    "    txt_feat = clip_model.encode_text(toks)\n",
    "    txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    sims = (img_feat @ txt_feat.T).squeeze(0)      # (num_cands,)\n",
    "    sims = sims.detach().float().cpu().tolist()\n",
    "    # Get best\n",
    "    best_idx = max(range(len(candidates)), key=lambda i: sims[i])\n",
    "    best_caption = candidates[best_idx]\n",
    "    ranked = sorted(zip(candidates, sims), key=lambda x: x[1], reverse=True)\n",
    "    return best_caption, ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4771fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning + CLIP rerank:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning + CLIP rerank:   0%|          | 0/2 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m pil = Image.open(fpath).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 1) generate N candidates with CoCa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m cands = \u001b[43mgenerate_n_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal candidates:\u001b[39m\u001b[33m\"\u001b[39m, cands)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cands:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mgenerate_n_candidates\u001b[39m\u001b[34m(pil_img, seq_len, temperature, N)\u001b[39m\n\u001b[32m     26\u001b[39m     pretrained_tag = \u001b[33m\"\u001b[39m\u001b[33mmscoco_finetuned_laion2b_s13b_b90k\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m cap = \u001b[43mgenerate_caption_openclip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpil_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# or seq_len=seq_len in your wrapper, both handled\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# >1.0 = more diverse; <1.0 = safer\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_tag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m cands.append(cap)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCandidate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcap\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mgenerate_caption_openclip\u001b[39m\u001b[34m(pil_img, max_len, temperature, model_name, pretrained_tag, top_k, top_p)\u001b[39m\n\u001b[32m     16\u001b[39m tokenizer = open_clip.get_tokenizer(model_name)\n\u001b[32m     17\u001b[39m model = model.to(device).eval()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m img = \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil_img\u001b[49m\u001b[43m)\u001b[49m.unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m     21\u001b[39m tried = []\n\u001b[32m     22\u001b[39m out = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:167\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[32m    166\u001b[39m mode_to_nptype = {\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: np.int32, \u001b[33m\"\u001b[39m\u001b[33mI;16\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.byteorder == \u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mI;16B\u001b[39m\u001b[33m\"\u001b[39m: np.int16, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m: np.float32}\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m img = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    170\u001b[39m     img = \u001b[32m255\u001b[39m * img\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "N = 4           # number of candidates per image\n",
    "SEQ_LEN = 28     # caption length\n",
    "TEMP = 0.9       # diversity\n",
    "\n",
    "preds = []\n",
    "missing = []\n",
    "all_candidates_debug = []   # optional: keep all N + scores per image for analysis\n",
    "ann[\"images\"] = ann[\"images\"][:2]\n",
    "\n",
    "for img_info in tqdm(ann[\"images\"], desc=\"Captioning + CLIP rerank\"):\n",
    "    image_id = img_info[\"id\"]\n",
    "    fpath = Path(IMG_DIR) / img_info[\"file_name\"]\n",
    "    if not fpath.exists():\n",
    "        missing.append(img_info[\"file_name\"]); continue\n",
    "\n",
    "    pil = Image.open(fpath).convert(\"RGB\")\n",
    "\n",
    "    # 1) generate N candidates with CoCa\n",
    "    cands = generate_n_candidates(pil, seq_len=SEQ_LEN, temperature=TEMP, N=N)\n",
    "    print(\"Final candidates:\", cands)\n",
    "    if not cands:\n",
    "        cands = [generate_caption_openclip(pil, max_len=SEQ_LEN, temperature=TEMP, N=N)]\n",
    "\n",
    "    # 2) CLIP rerank\n",
    "    # best_cap, ranked = clip_rerank(pil, cands)\n",
    "\n",
    "    preds.append({\"file_name\": img_info[\"file_name\"],\"image_id\": image_id, \"caption\": cands})\n",
    "    all_candidates_debug.append({\n",
    "        \"file_name\": img_info[\"file_name\"],\n",
    "        \"image_id\": image_id,\n",
    "        \"file_name\": img_info[\"file_name\"],\n",
    "        # \"ranked\": [{\"caption\": c, \"clipscore\": s} for c, s in ranked]\n",
    "    })\n",
    "\n",
    "len(preds), len(all_candidates_debug), len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: preds_nocaps_val_openclip.json\n"
     ]
    }
   ],
   "source": [
    "# --- Save predictions ---\n",
    "OUT_JSON = \"preds_nocaps_val_openclip.json\"\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(preds, f)\n",
    "print(\"Saved:\", OUT_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7dd11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "088eb835",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %pip install clip-by-openai\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclip\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'clip'"
     ]
    }
   ],
   "source": [
    "# %pip install clip-by-openai\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"data/validation/0013ea2087020901.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"this photo is one of the first photos i have of my great - great - great great great great great great great great great great great \",\n",
    "            \"a little boy that is standing up with a bat\",\n",
    "            \"1 9 5 0 - 0 4 - 0 1 - baby - in - front - of - house - 0 1 . jpg\",\n",
    "            \"an old black and white photo of a little boy\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "# Load GT and predictions\n",
    "coco = COCO(ANN_PATH)\n",
    "cocoRes = coco.loadRes(OUT_JSON)\n",
    "\n",
    "evaluator = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "# Replace the default scorers (which includes SPICE)\n",
    "evaluator.scorers = [\n",
    "    (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "    (Meteor(), \"METEOR\"),\n",
    "    (Rouge(), \"ROUGE_L\"),\n",
    "    (Cider(), \"CIDEr\"),\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluator.evaluate()\n",
    "\n",
    "print(\"\\n=== NoCaps-val (overall, no SPICE) ===\")\n",
    "for k, v in evaluator.eval.items():\n",
    "    print(f\"{k:10s}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Beam size 3â€“5 is good for CIDEr.\n",
    "- Max caption length ~20â€“30 tokens.\n",
    "- SPICE metric requires Java.\n",
    "- Leaderboard results differ (use online eval server for test split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca82229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
