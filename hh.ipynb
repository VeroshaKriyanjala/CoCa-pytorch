{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoCaps Validation — OpenCLIP CoCa End-to-End (Option A)\n",
    "\n",
    "This notebook:\n",
    "1. Installs dependencies (OpenCLIP CoCa + COCO caption eval).\n",
    "2. Loads NoCaps **validation** annotations (expects 10 captions per image).\n",
    "3. Loads a **pretrained CoCa** from OpenCLIP.\n",
    "4. Generates captions (beam search) for all images.\n",
    "5. Evaluates with **BLEU, METEOR, ROUGE_L, CIDEr** (SPICE optional).\n",
    "\n",
    "> **Paths to set:** `ANN_PATH` and `IMG_DIR` near the top.  \n",
    "> **Expected files:**  \n",
    ">  - `data/nocap_val_4500_captions.json`  \n",
    ">  - `data/validation/<image files>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73874556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting open_clip_torch\n",
      "  Using cached open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (11.0.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (0.20.1+cu121)\n",
      "Collecting pycocotools\n",
      "  Using cached pycocotools-2.0.10-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: torch>=2.0 in ./.venv/lib/python3.12/site-packages (from open_clip_torch) (2.5.1+cu121)\n",
      "Collecting regex (from open_clip_torch)\n",
      "  Using cached regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting ftfy (from open_clip_torch)\n",
      "  Using cached ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting huggingface-hub (from open_clip_torch)\n",
      "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors (from open_clip_torch)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting timm>=1.0.17 (from open_clip_torch)\n",
      "  Using cached timm-1.0.20-py3-none-any.whl.metadata (61 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->open_clip_torch) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0->open_clip_torch) (1.3.0)\n",
      "Collecting pyyaml (from timm>=1.0.17->open_clip_torch)\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from ftfy->open_clip_torch) (0.2.14)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
      "Collecting requests (from huggingface-hub->open_clip_torch)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub->open_clip_torch)\n",
      "  Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->huggingface-hub->open_clip_torch)\n",
      "  Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub->open_clip_torch)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub->open_clip_torch)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub->open_clip_torch)\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached pycocotools-2.0.10-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (397 kB)\n",
      "Using cached timm-1.0.20-py3-none-any.whl (2.5 MB)\n",
      "Using cached ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Using cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
      "Using cached regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, pycocotools, idna, hf-xet, ftfy, charset_normalizer, certifi, requests, huggingface-hub, timm, open_clip_torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [open_clip_torch] [open_clip_torch]\n",
      "\u001b[1A\u001b[2KSuccessfully installed certifi-2025.10.5 charset_normalizer-3.4.3 ftfy-6.3.1 hf-xet-1.1.10 huggingface-hub-0.35.3 idna-3.10 open_clip_torch-3.2.0 pycocotools-2.0.10 pyyaml-6.0.3 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 timm-1.0.20 tqdm-4.67.1 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
      "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-v5hxx2fd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-v5hxx2fd\n",
      "  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in ./.venv/lib/python3.12/site-packages (from pycocoevalcap==1.2) (2.0.10)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (2.1.2)\n",
      "Building wheels for collected packages: pycocoevalcap\n",
      "  Building wheel for pycocoevalcap (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312292 sha256=41dc5e4549bfe6e860cdc5695de4d5d7ef31f0f2f484d074c53edcdd6232eea0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6wef6k5v/wheels/03/ce/0b/3d3fdeecb09b4f4ebcfb3ff28d27a9f5b3c1a7b73897ad122d\n",
      "Successfully built pycocoevalcap\n",
      "Installing collected packages: pycocoevalcap\n",
      "Successfully installed pycocoevalcap-1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# --- Install dependencies (internet required) ---\n",
    "%pip install --upgrade pip\n",
    "%pip install open_clip_torch pillow tqdm torchvision pycocotools\n",
    "%pip install git+https://github.com/salaniz/pycocoevalcap\n",
    "# Optional for SPICE (Java required):\n",
    "# !apt-get update && apt-get install -y default-jre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31a0970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse_g3/CoCa-pytorch/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import open_clip\n",
    "from torchvision import transforms\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "# ---- Set your paths here ----\n",
    "ANN_PATH = \"data/nocap_val_4500_captions.json\"   # NoCaps validation annotations\n",
    "IMG_DIR  = \"data/validation\"                      # folder containing validation images\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "assert Path(ANN_PATH).exists(), f\"Annotation file not found: {ANN_PATH}\"\n",
    "assert Path(IMG_DIR).exists(), f\"Image folder not found: {IMG_DIR}\"\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a01dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# images: 4500\n",
      "min refs: 10, max refs: 10, mean refs: 10.00\n",
      "non-10 reference counts: 0\n",
      "Example image: 0013ea2087020901.jpg\n",
      "Refs: ['A baby is standing in front of a house.', 'A little girl in a white jacket and sandals.', 'A young child stands in front of a house.', 'A child is wearing a white shirt and standing on a side walk. ', 'A little boy is standing in his diaper with a white shirt on.', 'A child wearing a diaper and shoes stands on the sidewalk.', 'A child is wearing a light-colored shirt during the daytime.', 'A little kid standing on the pavement in a shirt. ', 'Black and white photo of a little girl smiling.', 'a cute baby is standing alone with white shirt']\n"
     ]
    }
   ],
   "source": [
    "# --- Load annotations + verify 10 refs per image ---\n",
    "with open(ANN_PATH, \"r\") as f:\n",
    "    ann = json.load(f)\n",
    "\n",
    "id2file = {img[\"id\"]: img[\"file_name\"] for img in ann[\"images\"]}\n",
    "caps_by_id = defaultdict(list)\n",
    "for a in ann[\"annotations\"]:\n",
    "    caps_by_id[a[\"image_id\"]].append(a[\"caption\"])\n",
    "\n",
    "num_images = len(ann[\"images\"])\n",
    "lens = [len(caps_by_id[i[\"id\"]]) for i in ann[\"images\"]]\n",
    "\n",
    "print(f\"# images: {num_images}\")\n",
    "print(f\"min refs: {min(lens)}, max refs: {max(lens)}, mean refs: {sum(lens)/len(lens):.2f}\")\n",
    "\n",
    "bad = [(i[\"id\"], id2file[i[\"id\"]], len(caps_by_id[i[\"id\"]])) for i in ann[\"images\"] if len(caps_by_id[i[\"id\"]]) != 10]\n",
    "print(\"non-10 reference counts:\", len(bad))\n",
    "if not bad:\n",
    "    first = ann[\"images\"][0]\n",
    "    print(\"Example image:\", first[\"file_name\"])\n",
    "    print(\"Refs:\", caps_by_id[first[\"id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae616ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-32', 'datacomp_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_clip_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_laion_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_image_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_text_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_basic_s128m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_m_s128m_b4k'),\n",
       " ('ViT-B-32', 'datacomp_s_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_clip_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_laion_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_image_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_text_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_basic_s13m_b4k'),\n",
       " ('ViT-B-32', 'commonpool_s_s13m_b4k'),\n",
       " ('ViT-B-32', 'metaclip_400m'),\n",
       " ('ViT-B-32', 'metaclip_fullcc'),\n",
       " ('ViT-B-32-256', 'datacomp_s34b_b86k'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
       " ('ViT-B-16', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-B-16', 'datacomp_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_clip_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_laion_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_image_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_text_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_basic_s1b_b8k'),\n",
       " ('ViT-B-16', 'commonpool_l_s1b_b8k'),\n",
       " ('ViT-B-16', 'dfn2b'),\n",
       " ('ViT-B-16', 'metaclip_400m'),\n",
       " ('ViT-B-16', 'metaclip_fullcc'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14', 'datacomp_xl_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_clip_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_laion_s13b_b90k'),\n",
       " ('ViT-L-14', 'commonpool_xl_s13b_b90k'),\n",
       " ('ViT-L-14', 'metaclip_400m'),\n",
       " ('ViT-L-14', 'metaclip_fullcc'),\n",
       " ('ViT-L-14', 'dfn2b'),\n",
       " ('ViT-L-14', 'dfn2b_s39b'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-H-14', 'metaclip_fullcc'),\n",
       " ('ViT-H-14', 'metaclip_altogether'),\n",
       " ('ViT-H-14', 'dfn5b'),\n",
       " ('ViT-H-14-378', 'dfn5b'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('ViT-g-14', 'laion2b_s34b_b88k'),\n",
       " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
       " ('ViT-bigG-14', 'metaclip_fullcc'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
       " ('convnext_base', 'laion400m_s13b_b51k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
       " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
       " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
       " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
       " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n",
       " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n",
       " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n",
       " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
       " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
       " ('EVA01-g-14', 'laion400m_s11b_b41k'),\n",
       " ('EVA01-g-14-plus', 'merged2b_s11b_b114k'),\n",
       " ('EVA02-B-16', 'merged2b_s8b_b131k'),\n",
       " ('EVA02-L-14', 'merged2b_s4b_b131k'),\n",
       " ('EVA02-L-14-336', 'merged2b_s6b_b61k'),\n",
       " ('EVA02-E-14', 'laion2b_s4b_b115k'),\n",
       " ('EVA02-E-14-plus', 'laion2b_s9b_b144k'),\n",
       " ('ViT-B-16-SigLIP', 'webli'),\n",
       " ('ViT-B-16-SigLIP-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-i18n-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP-384', 'webli'),\n",
       " ('ViT-B-16-SigLIP-512', 'webli'),\n",
       " ('ViT-L-16-SigLIP-256', 'webli'),\n",
       " ('ViT-L-16-SigLIP-384', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP-i18n-256', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP-378', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP-384', 'webli'),\n",
       " ('ViT-B-32-SigLIP2-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP2', 'webli'),\n",
       " ('ViT-B-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-B-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-B-16-SigLIP2-512', 'webli'),\n",
       " ('ViT-L-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-L-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-L-16-SigLIP2-512', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP2', 'webli'),\n",
       " ('ViT-SO400M-14-SigLIP2-378', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-SO400M-16-SigLIP2-512', 'webli'),\n",
       " ('ViT-gopt-16-SigLIP2-256', 'webli'),\n",
       " ('ViT-gopt-16-SigLIP2-384', 'webli'),\n",
       " ('ViT-L-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-L-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-H-14-CLIPA-336', 'laion2b'),\n",
       " ('ViT-H-14-CLIPA-336', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA', 'datacomp1b'),\n",
       " ('ViT-bigG-14-CLIPA-336', 'datacomp1b'),\n",
       " ('nllb-clip-base', 'v1'),\n",
       " ('nllb-clip-large', 'v1'),\n",
       " ('nllb-clip-base-siglip', 'v1'),\n",
       " ('nllb-clip-base-siglip', 'mrl'),\n",
       " ('nllb-clip-large-siglip', 'v1'),\n",
       " ('nllb-clip-large-siglip', 'mrl'),\n",
       " ('MobileCLIP-S1', 'datacompdr'),\n",
       " ('MobileCLIP-S2', 'datacompdr'),\n",
       " ('MobileCLIP-B', 'datacompdr'),\n",
       " ('MobileCLIP-B', 'datacompdr_lt'),\n",
       " ('MobileCLIP2-B', 'dfndr2b'),\n",
       " ('MobileCLIP2-S0', 'dfndr2b'),\n",
       " ('MobileCLIP2-S2', 'dfndr2b'),\n",
       " ('MobileCLIP2-S3', 'dfndr2b'),\n",
       " ('MobileCLIP2-S4', 'dfndr2b'),\n",
       " ('MobileCLIP2-L-14', 'dfndr2b'),\n",
       " ('ViTamin-S', 'datacomp1b'),\n",
       " ('ViTamin-S-LTT', 'datacomp1b'),\n",
       " ('ViTamin-B', 'datacomp1b'),\n",
       " ('ViTamin-B-LTT', 'datacomp1b'),\n",
       " ('ViTamin-L', 'datacomp1b'),\n",
       " ('ViTamin-L-256', 'datacomp1b'),\n",
       " ('ViTamin-L-336', 'datacomp1b'),\n",
       " ('ViTamin-L-384', 'datacomp1b'),\n",
       " ('ViTamin-L2', 'datacomp1b'),\n",
       " ('ViTamin-L2-256', 'datacomp1b'),\n",
       " ('ViTamin-L2-336', 'datacomp1b'),\n",
       " ('ViTamin-L2-384', 'datacomp1b'),\n",
       " ('ViTamin-XL-256', 'datacomp1b'),\n",
       " ('ViTamin-XL-336', 'datacomp1b'),\n",
       " ('ViTamin-XL-384', 'datacomp1b'),\n",
       " ('PE-Core-T-16-384', 'meta'),\n",
       " ('PE-Core-S-16-384', 'meta'),\n",
       " ('PE-Core-B-16', 'meta'),\n",
       " ('PE-Core-L-14-336', 'meta'),\n",
       " ('PE-Core-bigG-14-448', 'meta'),\n",
       " ('ViT-H-14-worldwide', 'metaclip2_worldwide'),\n",
       " ('ViT-H-14-worldwide-378', 'metaclip2_worldwide'),\n",
       " ('ViT-bigG-14-worldwide', 'metaclip2_worldwide'),\n",
       " ('ViT-bigG-14-worldwide-378', 'metaclip2_worldwide'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4-quickgelu', 'openai'),\n",
       " ('RN50x16-quickgelu', 'openai'),\n",
       " ('RN50x64-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-32-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-B-16-quickgelu', 'openai'),\n",
       " ('ViT-B-16-quickgelu', 'dfn2b'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-B-16-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-L-14-quickgelu', 'openai'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_400m'),\n",
       " ('ViT-L-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-L-14-quickgelu', 'dfn2b'),\n",
       " ('ViT-L-14-336-quickgelu', 'openai'),\n",
       " ('ViT-H-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-H-14-quickgelu', 'dfn5b'),\n",
       " ('ViT-H-14-378-quickgelu', 'dfn5b'),\n",
       " ('ViT-bigG-14-quickgelu', 'metaclip_fullcc'),\n",
       " ('ViT-H-14-worldwide-quickgelu', 'metaclip2_worldwide')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open_clip\n",
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad2ae17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Load OpenCLIP CoCa ---\n",
    "\n",
    "# ''' \n",
    "# ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
    "# ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
    "# ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
    "# ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
    "# '''\n",
    "\n",
    "# model_name = \"coca_ViT-L-14\"\n",
    "# pretrained_tag = \"mscoco_finetuned_laion2b_s13b_b90k\"\n",
    "\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained_tag)\n",
    "# tokenizer = open_clip.get_tokenizer(model_name)\n",
    "# model = model.to(device).eval()\n",
    "\n",
    "# print(\"Loaded:\", model_name, \"/\", pretrained_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c530edce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting certifi\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: certifi\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.10.5\n",
      "    Uninstalling certifi-2025.10.5:\n",
      "      Successfully uninstalled certifi-2025.10.5\n",
      "Successfully installed certifi-2025.10.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse_g3/CoCa-pytorch/.venv/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fix CA certificate bundle issue for HTTPS downloads\n",
    "%pip install --force-reinstall certifi\n",
    "\n",
    "# A separate CLIP model for scoring (small & fast: ViT-B/32)\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"openai\"  # or \"laion2b_s34b_b79k\" if you prefer open LAION weights\n",
    ")\n",
    "clip_tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def clipscore(pil_img, caption: str) -> float:\n",
    "    # Encode image once\n",
    "    img = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_feat = clip_model.encode_image(img)\n",
    "        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Encode caption\n",
    "    txt = clip_tokenizer([caption]).to(device)\n",
    "    with torch.no_grad():\n",
    "        txt_feat = clip_model.encode_text(txt)\n",
    "        txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # cosine similarity\n",
    "    sim = (img_feat @ txt_feat.T).item()\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a15fad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    from open_clip import tokenizer as openclip_tok_mod\n",
    "    _have_openclip_decoder = hasattr(openclip_tok_mod, \"decode\")\n",
    "except Exception:\n",
    "    openclip_tok_mod = None\n",
    "    _have_openclip_decoder = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption_openclip(pil_img, max_len=30, temperature=1.0, model_name=None, pretrained_tag=None, top_k=None, top_p=None):\n",
    "    \n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained_tag)\n",
    "    tokenizer = open_clip.get_tokenizer(model_name)\n",
    "    model = model.to(device).eval()\n",
    "    \n",
    "    img = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    tried = []\n",
    "    out = None\n",
    "    for kwargs in (\n",
    "        dict(seq_len=max_len, temperature=temperature, top_k=top_k, top_p=top_p),\n",
    "        dict(seq_len=max_len, temperature=temperature),\n",
    "        dict(max_len=max_len, temperature=temperature),\n",
    "        dict(seq_len=max_len),\n",
    "        dict(max_len=max_len),\n",
    "        dict(),\n",
    "    ):\n",
    "        try:\n",
    "            out = model.generate(img, **{k: v for k, v in kwargs.items() if v is not None})\n",
    "            break\n",
    "        except TypeError as e:\n",
    "            tried.append(str(e))\n",
    "            out = None\n",
    "\n",
    "    if out is None:\n",
    "        raise RuntimeError(\"open_clip CoCa.generate() signature not recognized. Tried:\\n\" + \"\\n\".join(tried))\n",
    "\n",
    "    # --- decode handling ---\n",
    "    if isinstance(out, list):\n",
    "        if len(out) and isinstance(out[0], str):\n",
    "            return out[0]\n",
    "        if len(out) and torch.is_tensor(out[0]):\n",
    "            ids = out[0]\n",
    "        elif len(out) and isinstance(out[0], (list, tuple)):\n",
    "            ids = torch.tensor(out[0])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unexpected list return type from model.generate(): {type(out[0])}\")\n",
    "    elif torch.is_tensor(out):\n",
    "        ids = out[0]\n",
    "    else:\n",
    "        return str(out)\n",
    "\n",
    "    if not torch.is_tensor(ids):\n",
    "        ids = torch.tensor(ids)\n",
    "    if _have_openclip_decoder:\n",
    "        return openclip_tok_mod.decode(ids)\n",
    "    if hasattr(model, \"tokenizer\") and hasattr(model.tokenizer, \"decode\"):\n",
    "        return model.tokenizer.decode(ids.tolist())\n",
    "\n",
    "    raise RuntimeError(\"model.generate returned token IDs but no decoder is available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a1ffdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_n_candidates(pil_img, seq_len=28, temperature=0.9, N=4):\n",
    "    \"\"\"\n",
    "    Calls your existing generate_caption_openclip N times to get diverse candidates.\n",
    "    NOTE: If your build is purely greedy, multiple calls may be identical.\n",
    "    Diversity relies on temperature / stochastic decoding in your OpenCLIP build.\n",
    "    \"\"\"\n",
    "    cands = []\n",
    "    for _ in range(N):\n",
    "        if _ == 0:\n",
    "            model_name = \"coca_ViT-B-32\"\n",
    "            pretrained_tag = \"laion2b_s13b_b90k\"\n",
    "            print(\"1\")\n",
    "        elif _ == 1:\n",
    "            model_name = \"coca_ViT-B-32\"\n",
    "            pretrained_tag = \"mscoco_finetuned_laion2b_s13b_b90k\"\n",
    "            print(\"2\")\n",
    "        elif _ == 2:\n",
    "            model_name = \"coca_ViT-L-14\"\n",
    "            pretrained_tag = \"laion2b_s13b_b90k\"\n",
    "            print(\"3\")\n",
    "        else:\n",
    "            model_name = \"coca_ViT-L-14\"\n",
    "            pretrained_tag = \"mscoco_finetuned_laion2b_s13b_b90k\"\n",
    "            print(\"4\")\n",
    "            \n",
    "        cap = generate_caption_openclip(\n",
    "            pil_img,\n",
    "            max_len=seq_len,         # or seq_len=seq_len in your wrapper, both handled\n",
    "            temperature=temperature,  # >1.0 = more diverse; <1.0 = safer\n",
    "            model_name=model_name,\n",
    "            pretrained_tag=pretrained_tag,\n",
    "        )\n",
    "        cands.append(cap)\n",
    "        print(f\"Candidate {_}: {cap}\")\n",
    "    # Deduplicate while keeping order\n",
    "    seen = set(); uniq = []\n",
    "    for c in cands:\n",
    "        if c not in seen:\n",
    "            seen.add(c); uniq.append(c)\n",
    "    return uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93839585",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clip_rerank(pil_img, candidates):\n",
    "    # Cache image feature once\n",
    "    img = clip_preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    img_feat = clip_model.encode_image(img)\n",
    "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Encode all captions together (batched)\n",
    "    if not candidates:\n",
    "        return None, []\n",
    "    toks = clip_tokenizer(candidates).to(device)\n",
    "    txt_feat = clip_model.encode_text(toks)\n",
    "    txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    sims = (img_feat @ txt_feat.T).squeeze(0)      # (num_cands,)\n",
    "    sims = sims.detach().float().cpu().tolist()\n",
    "    # Get best\n",
    "    best_idx = max(range(len(candidates)), key=lambda i: sims[i])\n",
    "    best_caption = candidates[best_idx]\n",
    "    ranked = sorted(zip(candidates, sims), key=lambda x: x[1], reverse=True)\n",
    "    return best_caption, ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4771fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning + CLIP rerank:   0%|          | 0/4500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning + CLIP rerank:   0%|          | 0/4500 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Please install transformers for generate functionality. `pip install transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m pil = Image.open(fpath).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 1) generate N candidates with CoCa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m cands = \u001b[43mgenerate_n_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal candidates:\u001b[39m\u001b[33m\"\u001b[39m, cands)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cands:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mgenerate_n_candidates\u001b[39m\u001b[34m(pil_img, seq_len, temperature, N)\u001b[39m\n\u001b[32m     26\u001b[39m     pretrained_tag = \u001b[33m\"\u001b[39m\u001b[33mmscoco_finetuned_laion2b_s13b_b90k\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m cap = \u001b[43mgenerate_caption_openclip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpil_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# or seq_len=seq_len in your wrapper, both handled\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# >1.0 = more diverse; <1.0 = safer\u001b[39;49;00m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_tag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m cands.append(cap)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCandidate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcap\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mgenerate_caption_openclip\u001b[39m\u001b[34m(pil_img, max_len, temperature, model_name, pretrained_tag, top_k, top_p)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m kwargs \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mdict\u001b[39m(seq_len=max_len, temperature=temperature, top_k=top_k, top_p=top_p),\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mdict\u001b[39m(seq_len=max_len, temperature=temperature),\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     \u001b[38;5;28mdict\u001b[39m(),\n\u001b[32m     30\u001b[39m ):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/.venv/lib/python3.12/site-packages/open_clip/coca_model.py:308\u001b[39m, in \u001b[36mCoCa.generate\u001b[39m\u001b[34m(self, image, text, seq_len, max_seq_len, temperature, generation_type, top_p, top_k, pad_token_id, eos_token_id, sot_token_id, num_beams, num_beam_groups, min_seq_len, stopping_criteria, repetition_penalty, fixed_output_length)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    287\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    288\u001b[39m     image,\n\u001b[32m   (...)\u001b[39m\u001b[32m    306\u001b[39m     \u001b[38;5;66;03m# taking many ideas and components from HuggingFace GenerationMixin\u001b[39;00m\n\u001b[32m    307\u001b[39m     \u001b[38;5;66;03m# https://huggingface.co/docs/transformers/main/en/main_classes/text_generation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m _has_transformers, \u001b[33m\"\u001b[39m\u001b[33mPlease install transformers for generate functionality. `pip install transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m seq_len > min_seq_len, \u001b[33m\"\u001b[39m\u001b[33mseq_len must be larger than min_seq_len\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    310\u001b[39m     device = image.device\n",
      "\u001b[31mAssertionError\u001b[39m: Please install transformers for generate functionality. `pip install transformers`."
     ]
    }
   ],
   "source": [
    "N = 4           # number of candidates per image\n",
    "SEQ_LEN = 28     # caption length\n",
    "TEMP = 0.9       # diversity\n",
    "\n",
    "preds = []\n",
    "missing = []\n",
    "all_candidates_debug = []   # optional: keep all N + scores per image for analysis\n",
    "ann[\"images\"] = ann[\"images\"]\n",
    "\n",
    "for img_info in tqdm(ann[\"images\"], desc=\"Captioning + CLIP rerank\"):\n",
    "    image_id = img_info[\"id\"]\n",
    "    fpath = Path(IMG_DIR) / img_info[\"file_name\"]\n",
    "    if not fpath.exists():\n",
    "        missing.append(img_info[\"file_name\"]); continue\n",
    "\n",
    "    pil = Image.open(fpath).convert(\"RGB\")\n",
    "\n",
    "    # 1) generate N candidates with CoCa\n",
    "    cands = generate_n_candidates(pil, seq_len=SEQ_LEN, temperature=TEMP, N=N)\n",
    "    print(\"Final candidates:\", cands)\n",
    "    if not cands:\n",
    "        cands = [generate_caption_openclip(pil, max_len=SEQ_LEN, temperature=TEMP, N=N)]\n",
    "\n",
    "    # 2) CLIP rerank\n",
    "    # best_cap, ranked = clip_rerank(pil, cands)\n",
    "\n",
    "    preds.append({\"file_name\": img_info[\"file_name\"],\"image_id\": image_id, \"caption\": cands})\n",
    "    all_candidates_debug.append({\n",
    "        \"file_name\": img_info[\"file_name\"],\n",
    "        \"image_id\": image_id,\n",
    "        \"file_name\": img_info[\"file_name\"],\n",
    "        # \"ranked\": [{\"caption\": c, \"clipscore\": s} for c, s in ranked]\n",
    "    })\n",
    "\n",
    "len(preds), len(all_candidates_debug), len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a0e50ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers>=4.42\n",
      "  Using cached transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting tokenizers>=0.15\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (2025.9.18)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.42) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.42) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.42) (1.1.10)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.42) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.42) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.42) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.42) (2025.10.5)\n",
      "Using cached transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: tokenizers, transformers, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [accelerate]3\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.10.1 tokenizers-0.22.1 transformers-4.57.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# In a new cell (with internet)\n",
    "%pip install -U \"transformers>=4.42\" \"tokenizers>=0.15\" accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11b38912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'open_clip' from '/home/cse_g3/CoCa-pytorch/.venv/lib/python3.12/site-packages/open_clip/__init__.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import open_clip\n",
    "import open_clip.coca_model as coca_model\n",
    "\n",
    "import transformers  # just to ensure it's importable now\n",
    "importlib.reload(coca_model)  # refreshes _has_transformers = True\n",
    "importlib.reload(open_clip)   # (optional but safe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b799cf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: open_clip_torch in ./.venv/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (11.0.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.12/site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: pycocotools in ./.venv/lib/python3.12/site-packages (2.0.10)\n",
      "Requirement already satisfied: torch>=2.0 in ./.venv/lib/python3.12/site-packages (from open_clip_torch) (2.5.1+cu121)\n",
      "Requirement already satisfied: regex in ./.venv/lib/python3.12/site-packages (from open_clip_torch) (2025.9.18)\n",
      "Requirement already satisfied: ftfy in ./.venv/lib/python3.12/site-packages (from open_clip_torch) (6.3.1)\n",
      "Requirement already satisfied: huggingface-hub in ./.venv/lib/python3.12/site-packages (from open_clip_torch) (0.35.3)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.12/site-packages (from open_clip_torch) (0.6.2)\n",
      "Requirement already satisfied: timm>=1.0.17 in ./.venv/lib/python3.12/site-packages (from open_clip_torch) (1.0.20)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0->open_clip_torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->open_clip_torch) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from ftfy->open_clip_torch) (0.2.14)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub->open_clip_torch) (1.1.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
      "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-5ykyulhm\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-5ykyulhm\n",
      "  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in ./.venv/lib/python3.12/site-packages (from pycocoevalcap==1.2) (2.0.10)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers>=4.42 in ./.venv/lib/python3.12/site-packages (4.57.0)\n",
      "Requirement already satisfied: tokenizers>=0.15 in ./.venv/lib/python3.12/site-packages (0.22.1)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (2025.9.18)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers>=4.42) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.42) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.42) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.42) (1.1.10)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.12/site-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.42) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.42) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.42) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers>=4.42) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# --- Install dependencies (internet required) ---\n",
    "%pip install --upgrade pip\n",
    "%pip install open_clip_torch pillow tqdm torchvision pycocotools\n",
    "%pip install git+https://github.com/salaniz/pycocoevalcap\n",
    "%pip install -U \"transformers>=4.42\" \"tokenizers>=0.15\" accelerate\n",
    "# Optional for SPICE (Java required):\n",
    "# !apt-get update && apt-get install -y default-jre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d100de81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Captioning (+optional CLIP rerank):   0%|          | 0/4500 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Please install transformers for generate functionality. `pip install transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Image.open(fpath).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pil:\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# 1) generate N candidates (no dedupe)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     cands = \u001b[43mgenerate_n_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;66;03m# 2) (Optional) CLIP rerank just for debugging/inspection, not changing the saved list\u001b[39;00m\n\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# best_cap, ranked = clip_rerank(pil, cands)\u001b[39;00m\n\u001b[32m    112\u001b[39m \n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Save ALL 4 captions for this image\u001b[39;00m\n\u001b[32m    114\u001b[39m preds.append({\n\u001b[32m    115\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfile_name\u001b[39m\u001b[33m\"\u001b[39m: img_info[\u001b[33m\"\u001b[39m\u001b[33mfile_name\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage_id\u001b[39m\u001b[33m\"\u001b[39m: image_id,\n\u001b[32m    117\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcaptions\u001b[39m\u001b[33m\"\u001b[39m: cands,   \u001b[38;5;66;03m# <-- list of 4 strings\u001b[39;00m\n\u001b[32m    118\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mgenerate_n_candidates\u001b[39m\u001b[34m(pil_img, seq_len, temperature, N)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[32m     79\u001b[39m     model_name, pretrained_tag = MODEL_SPECS[i % \u001b[38;5;28mlen\u001b[39m(MODEL_SPECS)]\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     cap = \u001b[43mgenerate_caption_openclip_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpil_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_tag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     caps.append(cap)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m caps\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mgenerate_caption_openclip_cached\u001b[39m\u001b[34m(pil_img, model_name, pretrained_tag, max_len, temperature, top_k, top_p)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m kwargs \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mdict\u001b[39m(seq_len=max_len, temperature=temperature, top_k=top_k, top_p=top_p),\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mdict\u001b[39m(seq_len=max_len, temperature=temperature),\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     \u001b[38;5;28mdict\u001b[39m(),\n\u001b[32m     34\u001b[39m ):\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CoCa-pytorch/.venv/lib/python3.12/site-packages/open_clip/coca_model.py:308\u001b[39m, in \u001b[36mCoCa.generate\u001b[39m\u001b[34m(self, image, text, seq_len, max_seq_len, temperature, generation_type, top_p, top_k, pad_token_id, eos_token_id, sot_token_id, num_beams, num_beam_groups, min_seq_len, stopping_criteria, repetition_penalty, fixed_output_length)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    287\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    288\u001b[39m     image,\n\u001b[32m   (...)\u001b[39m\u001b[32m    306\u001b[39m     \u001b[38;5;66;03m# taking many ideas and components from HuggingFace GenerationMixin\u001b[39;00m\n\u001b[32m    307\u001b[39m     \u001b[38;5;66;03m# https://huggingface.co/docs/transformers/main/en/main_classes/text_generation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m _has_transformers, \u001b[33m\"\u001b[39m\u001b[33mPlease install transformers for generate functionality. `pip install transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m seq_len > min_seq_len, \u001b[33m\"\u001b[39m\u001b[33mseq_len must be larger than min_seq_len\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    310\u001b[39m     device = image.device\n",
      "\u001b[31mAssertionError\u001b[39m: Please install transformers for generate functionality. `pip install transformers`."
     ]
    }
   ],
   "source": [
    "# ==== CACHED MODEL LOADER ====\n",
    "from functools import lru_cache\n",
    "\n",
    "MODEL_SPECS = [\n",
    "    (\"coca_ViT-B-32\", \"laion2b_s13b_b90k\"),\n",
    "    (\"coca_ViT-B-32\", \"mscoco_finetuned_laion2b_s13b_b90k\"),\n",
    "    (\"coca_ViT-L-14\", \"laion2b_s13b_b90k\"),\n",
    "    (\"coca_ViT-L-14\", \"mscoco_finetuned_laion2b_s13b_b90k\"),\n",
    "]\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def _load_openclip_model(model_name: str, pretrained_tag: str):\n",
    "    m, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained_tag)\n",
    "    tok = open_clip.get_tokenizer(model_name)\n",
    "    m = m.to(device).eval()\n",
    "    return m, tok, preprocess\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_caption_openclip_cached(pil_img, model_name: str, pretrained_tag: str,\n",
    "                                     max_len=30, temperature=1.0, top_k=None, top_p=None):\n",
    "    model, tokenizer, preprocess = _load_openclip_model(model_name, pretrained_tag)\n",
    "    img = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Try several argument names (OpenCLIP/Coca signatures vary by version)\n",
    "    tried = []\n",
    "    out = None\n",
    "    for kwargs in (\n",
    "        dict(seq_len=max_len, temperature=temperature, top_k=top_k, top_p=top_p),\n",
    "        dict(seq_len=max_len, temperature=temperature),\n",
    "        dict(max_len=max_len, temperature=temperature),\n",
    "        dict(seq_len=max_len),\n",
    "        dict(max_len=max_len),\n",
    "        dict(),\n",
    "    ):\n",
    "        try:\n",
    "            out = model.generate(img, **{k: v for k, v in kwargs.items() if v is not None})\n",
    "            break\n",
    "        except TypeError as e:\n",
    "            tried.append(str(e))\n",
    "            out = None\n",
    "\n",
    "    if out is None:\n",
    "        raise RuntimeError(\"open_clip CoCa.generate() signature not recognized. Tried:\\n\" + \"\\n\".join(tried))\n",
    "\n",
    "    # Normalize outputs to a string\n",
    "    if isinstance(out, list):\n",
    "        if len(out) and isinstance(out[0], str):\n",
    "            return out[0]\n",
    "        if len(out) and torch.is_tensor(out[0]):\n",
    "            ids = out[0]\n",
    "        elif len(out) and isinstance(out[0], (list, tuple)):\n",
    "            ids = torch.tensor(out[0])\n",
    "        else:\n",
    "            return str(out)\n",
    "    elif torch.is_tensor(out):\n",
    "        ids = out[0]\n",
    "    else:\n",
    "        return str(out)\n",
    "\n",
    "    # Decode token ids\n",
    "    if not torch.is_tensor(ids):\n",
    "        ids = torch.tensor(ids)\n",
    "    if _have_openclip_decoder:\n",
    "        return openclip_tok_mod.decode(ids)\n",
    "    if hasattr(model, \"tokenizer\") and hasattr(model.tokenizer, \"decode\"):\n",
    "        return model.tokenizer.decode(ids.tolist())\n",
    "\n",
    "    # Fallback: join token IDs as string (shouldn't happen in practice)\n",
    "    return \" \".join(map(str, ids.tolist()))\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_n_candidates(pil_img, seq_len=28, temperature=0.9, N=4):\n",
    "    \"\"\"\n",
    "    Generate exactly N captions (no deduping), using different model checkpoints\n",
    "    to encourage diversity. If N > len(MODEL_SPECS), we cycle through the list.\n",
    "    \"\"\"\n",
    "    caps = []\n",
    "    for i in range(N):\n",
    "        model_name, pretrained_tag = MODEL_SPECS[i % len(MODEL_SPECS)]\n",
    "        cap = generate_caption_openclip_cached(\n",
    "            pil_img,\n",
    "            model_name=model_name,\n",
    "            pretrained_tag=pretrained_tag,\n",
    "            max_len=seq_len,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        caps.append(cap)\n",
    "    return caps\n",
    "\n",
    "# ======= MAIN LOOP (always 4 captions per image) =======\n",
    "N = 4         # number of captions per image\n",
    "SEQ_LEN = 28  # caption length\n",
    "TEMP = 0.9    # sampling temperature\n",
    "\n",
    "preds = []\n",
    "missing = []\n",
    "all_candidates_debug = []\n",
    "\n",
    "for img_info in tqdm(ann[\"images\"], desc=\"Captioning (+optional CLIP rerank)\"):\n",
    "    image_id = img_info[\"id\"]\n",
    "    fpath = Path(IMG_DIR) / img_info[\"file_name\"]\n",
    "    if not fpath.exists():\n",
    "        missing.append(img_info[\"file_name\"])\n",
    "        continue\n",
    "\n",
    "    with Image.open(fpath).convert(\"RGB\") as pil:\n",
    "        # 1) generate N candidates (no dedupe)\n",
    "        cands = generate_n_candidates(pil, seq_len=SEQ_LEN, temperature=TEMP, N=N)\n",
    "\n",
    "        # 2) (Optional) CLIP rerank just for debugging/inspection, not changing the saved list\n",
    "        # best_cap, ranked = clip_rerank(pil, cands)\n",
    "\n",
    "    # Save ALL 4 captions for this image\n",
    "    preds.append({\n",
    "        \"file_name\": img_info[\"file_name\"],\n",
    "        \"image_id\": image_id,\n",
    "        \"captions\": cands,   # <-- list of 4 strings\n",
    "    })\n",
    "\n",
    "    # Optional debug block if you later re-enable reranking\n",
    "    all_candidates_debug.append({\n",
    "        \"file_name\": img_info[\"file_name\"],\n",
    "        \"image_id\": image_id,\n",
    "        # \"ranked\": [{\"caption\": c, \"clipscore\": s} for c, s in ranked],\n",
    "    })\n",
    "\n",
    "print(f\"Images processed: {len(preds)}; missing files: {len(missing)}\")\n",
    "\n",
    "# --- Save predictions ---\n",
    "OUT_JSON = \"preds_nocaps_val_openclip.json\"\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(preds, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved:\", OUT_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5328715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save predictions ---\n",
    "OUT_JSON = \"preds_nocaps_val_openclip.json\"\n",
    "with open(OUT_JSON, \"w\") as f:\n",
    "    json.dump(preds, f)\n",
    "print(\"Saved:\", OUT_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7dd11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088eb835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %pip install clip-by-openai\n",
    "\n",
    "# import clip\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# image = preprocess(Image.open(\"data/validation/0013ea2087020901.png\")).unsqueeze(0).to(device)\n",
    "# text = clip.tokenize([\"this photo is one of the first photos i have of my great - great - great great great great great great great great great great great \",\n",
    "#             \"a little boy that is standing up with a bat\",\n",
    "#             \"1 9 5 0 - 0 4 - 0 1 - baby - in - front - of - house - 0 1 . jpg\",\n",
    "#             \"an old black and white photo of a little boy\"]).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     image_features = model.encode_image(image)\n",
    "#     text_features = model.encode_text(text)\n",
    "    \n",
    "#     logits_per_image, logits_per_text = model(image, text)\n",
    "#     probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "# print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "# Load GT and predictions\n",
    "coco = COCO(ANN_PATH)\n",
    "cocoRes = coco.loadRes(OUT_JSON)\n",
    "\n",
    "evaluator = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "# Replace the default scorers (which includes SPICE)\n",
    "evaluator.scorers = [\n",
    "    (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "    (Meteor(), \"METEOR\"),\n",
    "    (Rouge(), \"ROUGE_L\"),\n",
    "    (Cider(), \"CIDEr\"),\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "evaluator.evaluate()\n",
    "\n",
    "print(\"\\n=== NoCaps-val (overall, no SPICE) ===\")\n",
    "for k, v in evaluator.eval.items():\n",
    "    print(f\"{k:10s}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Beam size 3–5 is good for CIDEr.\n",
    "- Max caption length ~20–30 tokens.\n",
    "- SPICE metric requires Java.\n",
    "- Leaderboard results differ (use online eval server for test split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca82229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
